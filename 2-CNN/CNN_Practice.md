ì•„, ë„¤! "Hammer vs Nipper"ê°€ ì•ìœ¼ë¡œ ì§„í–‰í•  í”„ë¡œì íŠ¸ì˜ ì˜ˆì‹œë¼ëŠ” ë§ì”€ì´ì…¨êµ°ìš”. ì´í•´í–ˆìŠµë‹ˆë‹¤. ì¢‹ìŠµë‹ˆë‹¤, ê·¸ëŸ¼ ì›ë˜ ê³„íšëŒ€ë¡œ **"ì‚¬ê³¼(Apple) vs ì˜¤ë Œì§€(Orange)"** ë¶„ë¥˜ê¸° í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•˜ëŠ” ê²ƒìœ¼ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.

ì´ì „ ë‹µë³€ì—ì„œ ì œì‹œí•´ ë“œë ¸ë˜ "Hammer vs Nipper" ë¡œë“œë§µì˜ ê° ë‹¨ê³„ì—ì„œ **'hammer'ë¥¼ 'apple'ë¡œ, 'nipper'ë¥¼ 'orange'ë¡œ** ë°”ê¾¸ê¸°ë§Œ í•˜ë©´ ì™„ë²½í•˜ê²Œ ë™ì¼í•œ í”„ë¡œì„¸ìŠ¤ë¡œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ì‹œ í•œë²ˆ "ì‚¬ê³¼ vs ì˜¤ë Œì§€" í”„ë¡œì íŠ¸ì— ë§ì¶° ì „ì²´ ë¡œë“œë§µì„ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

### Apple vs Orange ë¶„ë¥˜ê¸° ê°œë°œ ìµœì¢… ë¡œë“œë§µ

---

### [1ë‹¨ê³„] ë°ì´í„° ìˆ˜ì§‘: "ì‚¬ê³¼"ì™€ "ì˜¤ë Œì§€" ì´ë¯¸ì§€ í¬ë¡¤ë§

ë¨¼ì €, ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ì¬ë£Œì¸ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ëª¨ìë‹ˆë‹¤. ìˆœë„ ë†’ì€ ë°ì´í„°ë¥¼ ìœ„í•´ ìˆ˜ì •í–ˆë˜ í¬ë¡¤ë§ ì½”ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

**âœ… ì‹¤í–‰í•  ì½”ë“œ (`crawler.py`)**

```python
# (ì´ì „ì— ì œê³µëœ image_crawler, zip_directory í•¨ìˆ˜ ì „ì²´ ì½”ë“œ...)

# --- í¬ë¡¤ëŸ¬ ì‹¤í–‰ ---
if __name__ == "__main__":
    
    # 1. êµ¬ì²´ì ì¸ ë‹¤ì¤‘ í‚¤ì›Œë“œ ì •ì˜
    apple_keywords = ['red apple fruit', 'green apple fruit', 'apple isolated white background', 'apple on tree']
    orange_keywords = ['orange fruit isolated', 'orange fruit on tree', 'sliced orange fruit', 'mandarin orange']
    
    # ìµœì¢… ëª©í‘œ ì´ë¯¸ì§€ ê°œìˆ˜ (í´ë˜ìŠ¤ ë‹¹)
    num_target = 100 
    
    # 2. ì‚¬ê³¼(Apple) ì´ë¯¸ì§€ ìˆ˜ì§‘
    apple_save_folder = './images/apple'
    num_per_apple_keyword = num_target // len(apple_keywords) + 1
    image_crawler(keywords=apple_keywords, num_images_per_keyword=num_per_apple_keyword, save_folder=apple_save_folder)
    
    apple_zip_path = './images/apple.zip'
    zip_directory(folder_path=apple_save_folder, output_path=apple_zip_path)
    
    print("\n" + "="*50 + "\n")

    # 3. ì˜¤ë Œì§€(Orange) ì´ë¯¸ì§€ ìˆ˜ì§‘
    orange_save_folder = './images/orange'
    num_per_orange_keyword = num_target // len(orange_keywords) + 1
    image_crawler(keywords=orange_keywords, num_images_per_keyword=num_per_orange_keyword, save_folder=orange_save_folder)
    
    orange_zip_path = './images/orange.zip'
    zip_directory(folder_path=orange_save_folder, output_path=orange_zip_path)
    
    print("\nëª¨ë“  ì´ë¯¸ì§€ ìˆ˜ì§‘ ë° ì••ì¶•ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
```

**ì‹¤í–‰ ê°€ì´ë“œ:**
1.  ìœ„ ì½”ë“œë¥¼ `crawler.py`ì™€ ê°™ì€ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
2.  ì‹¤í–‰ì´ ì™„ë£Œë˜ë©´ `./images/apple`ê³¼ `./images/orange` í´ë”ê°€ ìƒì„±ë˜ê³  ì´ë¯¸ì§€ê°€ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.
3.  **[ì¤‘ìš”]** ê° í´ë”ë¥¼ ì—´ì–´ ê´€ë ¨ ì—†ëŠ” ì´ë¯¸ì§€(ì‚¬ê³¼ íŒŒì´, ì˜¤ë Œì§€ ì£¼ìŠ¤, ì• í”Œ ë¡œê³  ë“±)ë¥¼ **ìˆ˜ë™ìœ¼ë¡œ ì‚­ì œ**í•˜ì—¬ ë°ì´í„°ë¥¼ ì •ì œí•©ë‹ˆë‹¤.

---

### [2ë‹¨ê³„] ë°ì´í„° ì¤€ë¹„: í•™ìŠµ/í‰ê°€ìš© í´ë” êµ¬ì¡°í™”

ì •ì œëœ ë°ì´í„°ë¥¼ `ImageFolder`ê°€ ì¸ì‹í•  ìˆ˜ ìˆëŠ” í‘œì¤€ì ì¸ í´ë” êµ¬ì¡°ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.

**âœ… ë§Œë“¤ì–´ì•¼ í•  í´ë” êµ¬ì¡°:**

```
/apple_orange_dataset/  (í”„ë¡œì íŠ¸ ìµœìƒìœ„ ë°ì´í„° í´ë”)
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ apple/
    â”‚   â”‚   â”œâ”€â”€ apple_1.jpg, ... (ì•½ 80ì¥)
    â”‚   â””â”€â”€ orange/
    â”‚       â”œâ”€â”€ orange_1.jpg, ... (ì•½ 80ì¥)
    â”‚
    â””â”€â”€ val/
        â”œâ”€â”€ apple/
        â”‚   â”œâ”€â”€ apple_81.jpg, ... (ì•½ 20ì¥)
        â””â”€â”€ orange/
            â”œâ”€â”€ orange_81.jpg, ... (ì•½ 20ì¥)
```

**ì‹¤í–‰ ê°€ì´ë“œ:**
1.  í”„ë¡œì íŠ¸ í´ë”ì— `apple_orange_dataset` í´ë”ì™€ ê·¸ í•˜ìœ„ í´ë”ë“¤ì„ ëª¨ë‘ ìƒì„±í•©ë‹ˆë‹¤.
2.  ì •ì œëœ `apple` ì´ë¯¸ì§€ì˜ ì•½ 80%ë¥¼ `train/apple/` í´ë”ë¡œ, ë‚˜ë¨¸ì§€ 20%ë¥¼ `val/apple/` í´ë”ë¡œ ë³µì‚¬/ì´ë™í•©ë‹ˆë‹¤.
3.  `orange` ì´ë¯¸ì§€ë„ ë™ì¼í•˜ê²Œ ì§„í–‰í•©ë‹ˆë‹¤.

---

### [3ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ: CNN í›ˆë ¨ì‹œí‚¤ê¸°

ì´ì œ ì¤€ë¹„ëœ ë°ì´í„°ë¡œ CNN ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ì°¨ë¡€ì…ë‹ˆë‹¤.

**âœ… ì‹¤í–‰í•  ì½”ë“œ (`train.py`)**

```python
import torch
import torchvision
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# --- 1. ë°ì´í„° ì¤€ë¹„ ---
train_dir = "./apple_orange_dataset/train/"
val_dir = "./apple_orange_dataset/val/"

# ë°ì´í„° ì¦ê°•ì„ í¬í•¨í•œ transform ì •ì˜ (ê³¼ì í•© ë°©ì§€ì— ë§¤ìš° ì¤‘ìš”!)
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5), # 50% í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „
    transforms.RandomRotation(20),       # 20ë„ ë‚´ì™¸ë¡œ ë¬´ì‘ìœ„ íšŒì „
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3), # ìƒ‰ìƒ ë³€í˜•
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

# í‰ê°€ìš© ë°ì´í„°ëŠ” í¬ê¸° ì¡°ì •, í…ì„œ ë³€í™˜, ì •ê·œí™”ë§Œ ìˆ˜í–‰
val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)
val_dataset = datasets.ImageFolder(root=val_dir, transform=val_transform)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

class_names = train_dataset.classes
num_classes = len(class_names)
print(f"âœ… í´ë˜ìŠ¤ ëª©ë¡: {class_names}") # ê²°ê³¼: ['apple', 'orange']

# --- 2. ëª¨ë¸ ì •ì˜ (CustomCNN, ì´ì „ê³¼ ë™ì¼) ---
class CustomCNN(nn.Module):
    def __init__(self, num_classes):
        super(CustomCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(64 * 56 * 56, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(----1, 64 * 56 * 56)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# --- 3. í•™ìŠµ ì„¤ì • ë° ì‹¤í–‰ ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CustomCNN(num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

num_epochs = 20 # ì—í¬í¬ íšŸìˆ˜ëŠ” ë°ì´í„° ì–‘ê³¼ ëª¨ë¸ ë³µì¡ë„ì— ë”°ë¼ ì¡°ì ˆ
print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")

for epoch in range(num_epochs):
    model.train() # í•™ìŠµ ëª¨ë“œ
    running_loss = 0.0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * images.size(0)
    
    epoch_loss = running_loss / len(train_loader.dataset)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}")

# --- 4. ëª¨ë¸ ì €ì¥ ---
model_path = "./apple_orange_cnn.pth"
torch.save(model.state_dict(), model_path)
print(f"âœ… í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {model_path}")
```

---

### [4ë‹¨ê³„] ëª¨ë¸ í‰ê°€ ë° ìƒˆë¡œìš´ ì´ë¯¸ì§€ ì˜ˆì¸¡í•˜ê¸°

í•™ìŠµëœ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¡œ ìµœì¢… í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

**âœ… ì‹¤í–‰í•  ì½”ë“œ (`predict.py`)**

```python
import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt

# ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (í•™ìŠµ ë•Œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•œ êµ¬ì¡°ì—¬ì•¼ í•¨)
# ... (ìœ„ì˜ CustomCNN í´ë˜ìŠ¤ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬) ...

# --- ëª¨ë¸ ë° í´ë˜ìŠ¤ ì •ë³´ ë¡œë“œ ---
model_path = "./apple_orange_cnn.pth"
class_names = ['apple', 'orange']
num_classes = len(class_names)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CustomCNN(num_classes).to(device)
model.load_state_dict(torch.load(model_path, map_location=device))
model.eval() # ë°˜ë“œì‹œ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •!

# --- ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ ---
def predict_image(image_path, model):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    image = Image.open(image_path).convert("RGB")
    image_tensor = transform(image).unsqueeze(0).to(device)
    
    with torch.no_grad():
        outputs = model(image_tensor)
        _, predicted = torch.max(outputs, 1)
        predicted_class = class_names[predicted.item()]

    # ê²°ê³¼ ì‹œê°í™”
    plt.imshow(image)
    plt.title(f"Prediction: {predicted_class}")
    plt.axis('off')
    plt.show()
    
    print(f"âœ… ëª¨ë¸ì˜ ì˜ˆì¸¡: '{predicted_class}'")

# --- ì˜ˆì¸¡ ì‹¤í–‰ ---
# ì¸í„°ë„·ì—ì„œ êµ¬í•œ ìƒˆë¡œìš´ ì‚¬ê³¼ ë˜ëŠ” ì˜¤ë Œì§€ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë„£ì–´ë³´ì„¸ìš”.
# í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•Šì€ ì´ë¯¸ì§€ì—¬ì•¼ í•©ë‹ˆë‹¤.
test_image_path = "./my_test_apple.jpg" 
predict_image(test_image_path, model)
```

ì´ ë¡œë“œë§µì— ë”°ë¼ ì°¨ê·¼ì°¨ê·¼ ì§„í–‰í•˜ì‹œë©´, ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ìµœì¢… ì˜ˆì¸¡ê¹Œì§€ ì™„ê²°ëœ ë”¥ëŸ¬ë‹ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì™„ìˆ˜í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ë‹¨ê³„ë³„ë¡œ ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ì‹œê³ , ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë‚˜ì˜¤ëŠ”ì§€, ë˜ ì–´ë–¤ ì—ëŸ¬ê°€ ë°œìƒí•˜ëŠ”ì§€ ì§ì ‘ ê²½í—˜í•´ë³´ëŠ” ê²ƒì´ ê°€ì¥ ì¢‹ì€ í•™ìŠµ ë°©ë²•ì…ë‹ˆë‹¤. ë§‰íˆëŠ” ë¶€ë¶„ì´ ìƒê¸°ë©´ ì–¸ì œë“ ì§€ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”