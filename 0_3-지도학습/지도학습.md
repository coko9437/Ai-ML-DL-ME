
---

### **코드 종합 설명**

이 코드는 **지도 학습(Supervised Learning)**의 가장 대표적인 두 가지 문제 유형인 **회귀(Regression)**와 **분류(Classification)**를 PyTorch를 이용해 직접 구현하고 비교하는 예제입니다.

1.  **회귀 문제 (온도 예측):** 섭씨온도(`Celsius`)를 입력받아 화씨온도(`Fahrenheit`)를 예측하는 간단한 선형 회귀 모델을 만듭니다. 이 과정에서 지도 학습의 핵심 5단계(예측 → 손실 계산 → 역전파 → 최적화 → 반복)가 어떻게 코드로 구현되는지 명확하게 보여줍니다.
2.  **분류 문제 (붓꽃 품종 예측):** Scikit-learn 라이브러리에 내장된 붓꽃(Iris) 데이터를 사용하여, 꽃잎과 꽃받침의 측정치를 보고 3가지 품종 중 하나로 분류하는 다중 클래스 분류 모델을 만듭니다.
3.  **데이터 분할:** 두 예제 모두에서 `scikit-learn`의 `train_test_split` 함수를 사용하여 전체 데이터를 **학습용(train)**과 **평가용(test)**으로 나누는 과정을 보여줍니다. 이를 통해 모델이 학습하지 않은 새로운 데이터에 대해 얼마나 잘 작동하는지(일반화 성능)를 평가하는 것의 중요성을 강조합니다.

이 코드를 통해 "연속적인 숫자"를 예측하는 회귀와 "정해진 카테고리"를 맞추는 분류 문제의 차이점, 그리고 두 문제 모두에 공통적으로 적용되는 지도 학습의 기본 훈련 루프(Training Loop) 구조를 명확하게 이해할 수 있습니다.

---

### **Part 1: 회귀 (Regression) - 섭씨를 화씨로 변환하는 모델 만들기**

지도 학습의 가장 기본적인 형태인 회귀 문제를 통해 '훈련 루프'의 핵심 5단계를 코드로 구현하고 이해하는 과정입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
import numpy as np

# --- 1. 데이터 준비 및 분할 ---
# 섭씨(입력 X)와 화씨(정답 y) 데이터를 준비합니다. (화씨 = 섭씨 * 1.8 + 32)
celsius_q = np.array([-40, -10, 0, 8, 15, 22, 38], dtype=np.float32)
fahrenheit_a = np.array([-40, 14, 32, 46.4, 59, 71.6, 100.4], dtype=np.float32)

# 데이터를 PyTorch 텐서로 변환하고, 모델 입력에 맞게 차원을 변경합니다 (N -> N, 1).
# 모델은 (배치크기, 특성개수) 형태의 2차원 텐서를 기대하므로 차원을 추가해줍니다.
X = torch.from_numpy(celsius_q).view(-1, 1)
y = torch.from_numpy(fahrenheit_a).view(-1, 1)

# Scikit-learn을 이용해 데이터를 학습용(80%)과 평가용(20%)으로 무작위 분할합니다.
# random_state는 재현 가능성을 위해 무작위 시드를 고정하는 역할을 합니다.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)
print("학습 데이터 shape:", X_train.shape, y_train.shape)
print("평가 데이터 shape:", X_test.shape, y_test.shape)


# --- 2. 모델 정의 ---
# 가장 간단한 선형 모델을 정의합니다. 입력 특성 1개(섭씨), 출력 특성 1개(화씨).
# y = Wx + b 형태의 모델입니다.
model = nn.Linear(1, 1)


# --- 3. 손실 함수와 옵티마이저 정의 ---
# 회귀 문제에서 가장 일반적으로 사용되는 손실 함수인 평균 제곱 오차(Mean Squared Error)를 사용합니다.
criterion = nn.MSELoss()
# 모델의 가중치(W)와 편향(b)을 업데이트할 최적화 알고리즘으로 SGD(확률적 경사 하강법)를 사용합니다.
# model.parameters()는 모델이 학습해야 할 모든 파라미터(W, b)를 옵티마이저에게 알려줍니다.
# lr은 학습률(learning rate)로, 파라미터를 얼마나 크게 업데이트할지 결정합니다.
optimizer = optim.SGD(model.parameters(), lr=0.01)


# --- 4. 훈련 루프 (Training Loop) ---
epochs = 5000  # 전체 학습 데이터를 5000번 반복하여 학습합니다.
for epoch in range(epochs):
    # (1) 예측값 산출 (Forward Pass)
    # 현재 모델의 가중치(W, b)를 기반으로 예측값을 계산합니다.
    y_pred = model(X_train)

    # (2) 손실 계산 (Loss Calculation)
    # 예측값과 실제 정답(y_train) 간의 오차(손실)를 계산합니다.
    loss = criterion(y_pred, y_train)

    # (3) 기울기 초기화 (Gradient Zeroing)
    # 이전 반복에서 계산된 기울기 값이 누적되지 않도록 매번 0으로 초기화합니다.
    optimizer.zero_grad()

    # (4) 역전파 (Backpropagation)
    # 계산된 손실을 기준으로, 각 파라미터(W, b)가 손실에 얼마나 영향을 미쳤는지(기울기)를 계산합니다.
    loss.backward()

    # (5) 파라미터 업데이트 (Optimizer Step)
    # 계산된 기울기를 바탕으로 옵티마이저가 모델의 파라미터를 업데이트합니다.
    optimizer.step()

    if (epoch + 1) % 500 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# --- 5. 모델 평가 ---
# 학습이 끝난 모델을 평가 모드로 전환합니다 (이 모델은 간단해서 큰 의미는 없지만 좋은 습관입니다).
model.eval()

# 기울기 계산을 비활성화하여 불필요한 연산을 막습니다.
with torch.no_grad():
    # 학습에 사용하지 않은 '평가 데이터'를 이용해 예측을 수행합니다.
    test_pred = model(X_test)
    test_loss = criterion(test_pred, y_test)
    print(f'\n평가 데이터 손실: {test_loss.item():.4f}')

# 10도 섭씨를 예측해보기
predicted_fahrenheit = model(torch.tensor([[10.0]]))
print(f'\n섭씨 10도는 화씨 {predicted_fahrenheit.item():.2f}도로 예측됩니다.')
# 학습된 가중치(W)와 편향(b) 확인
# W는 1.8에, b는 32에 가까워져야 합니다.
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name, param.data)
```

#### **2. 해당 설명**

위 코드는 제공해주신 텍스트의 **지도 학습 흐름**을 코드로 완벽하게 구현한 것입니다. 학생 A가 문제집을 풀고 채점하며 실력을 쌓는 과정과 정확히 일치합니다.

*   **문제집/정답지:** `X_train`, `y_train`
*   **학생 A의 두뇌:** `model = nn.Linear(1, 1)`
*   **정답 작성 (예측):** `y_pred = model(X_train)`
*   **채점 (손실 계산):** `loss = criterion(y_pred, y_train)`
*   **오답 노트 및 지식 수정 (최적화):** `loss.backward()` 와 `optimizer.step()`

**데이터 분할**의 중요성도 명확히 보여줍니다. 모델은 `X_train` 데이터만 보고 학습했지만, 최종 평가는 학습 과정에서 한 번도 본 적 없는 `X_test` 데이터로 수행합니다. 이를 통해 모델이 단순히 문제집을 '암기'한 것인지, 아니면 섭씨와 화씨의 '관계'라는 일반적인 원리를 '학습'한 것인지(일반화 성능)를 객관적으로 평가할 수 있습니다.

#### **3. 응용 가능한 예제**

**"공부 시간에 따른 시험 점수 예측 모델"**

`X` 데이터를 '일일 공부 시간'으로, `y` 데이터를 '시험 점수'로 바꾸면, 공부 시간을 입력했을 때 예상 시험 점수를 예측하는 회귀 모델을 똑같은 구조로 만들 수 있습니다.

#### **4. 추가하고 싶은 내용 (모델 저장 및 로드)**

학습이 완료된 모델은 나중에 다시 사용하기 위해 저장할 수 있습니다. `torch.save(model.state_dict(), 'model_weights.pth')` 코드로 모델의 파라미터(가중치와 편향)를 저장하고, `model.load_state_dict(torch.load('model_weights.pth'))` 코드로 다시 불러와서 사용할 수 있습니다.

#### **5. 심화 내용 (경사 하강법과 학습률의 의미)**

옵티마이저가 파라미터를 업데이트하는 원리가 바로 **경사 하강법(Gradient Descent)**입니다. 손실 함수라는 거대한 '산'에서 가장 낮은 '계곡' 지점을 찾아가는 과정과 같습니다. `loss.backward()`는 현재 위치에서 가장 가파른 내리막길의 '방향'(기울기)을 알려주고, **학습률(learning rate)**은 그 방향으로 얼마나 큰 '보폭'으로 나아갈지를 결정합니다. 학습률이 너무 크면 계곡을 지나쳐 버릴 수 있고, 너무 작으면 학습이 매우 느려지기 때문에 적절한 값을 찾는 것이 매우 중요합니다.

---

### **Part 2: 분류 (Classification) - 붓꽃(Iris) 품종 분류하기**

지도 학습의 또 다른 핵심 문제인 분류를 통해, 회귀와의 차이점(모델 구조, 손실 함수)을 이해하는 과정입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# --- 1. 데이터 준비 및 분할 ---
# Scikit-learn에 내장된 붓꽃(Iris) 데이터셋을 불러옵니다.
iris = load_iris()
X, y = iris.data, iris.target

# 데이터를 학습용(80%)과 평가용(20%)으로 분할합니다.
# stratify=y 옵션은 분할된 데이터셋에서도 원본 데이터의 클래스 비율(품종 비율)을 유지시켜줍니다.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 데이터의 스케일을 표준화합니다. 이는 모델의 학습을 더 안정적이고 빠르게 만듭니다.
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Numpy 배열을 PyTorch 텐서로 변환합니다.
# 분류 문제의 정답(레이블)은 보통 LongTensor 타입을 사용합니다.
X_train = torch.from_numpy(X_train.astype(np.float32))
X_test = torch.from_numpy(X_test.astype(np.float32))
y_train = torch.from_numpy(y_train).type(torch.LongTensor)
y_test = torch.from_numpy(y_test).type(torch.LongTensor)


# --- 2. 모델 정의 ---
# 붓꽃 데이터는 4개의 특성(꽃받침/꽃잎의 길이/너비)을 입력받아 3개의 품종 중 하나로 분류합니다.
# nn.Sequential은 여러 개의 계층(layer)을 순서대로 쌓아 모델을 만드는 간편한 방법입니다.
model = nn.Sequential(
    nn.Linear(4, 16),  # 입력 4개 -> 은닉층 16개
    nn.ReLU(),         # 활성화 함수
    nn.Linear(16, 3)   # 은닉층 16개 -> 출력 3개 (3개 품종에 대한 점수)
)


# --- 3. 손실 함수와 옵티마이저 정의 ---
# 다중 클래스 분류 문제에서 가장 일반적으로 사용되는 손실 함수인 교차 엔트로피(Cross-Entropy)를 사용합니다.
# nn.CrossEntropyLoss는 내부적으로 Softmax 함수를 포함하고 있어 모델의 마지막에 Softmax를 추가할 필요가 없습니다.
criterion = nn.CrossEntropyLoss()
# 이번에는 더 성능이 좋다고 알려진 Adam 옵티마이저를 사용해봅니다.
optimizer = optim.Adam(model.parameters(), lr=0.01)


# --- 4. 훈련 루프 (Training Loop) ---
# 회귀 문제와 훈련 루프의 구조는 완전히 동일합니다.
epochs = 200
for epoch in range(epochs):
    y_pred = model(X_train)
    loss = criterion(y_pred, y_train)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 20 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')


# --- 5. 모델 평가 ---
model.eval()
with torch.no_grad():
    # 모델의 출력은 각 클래스에 대한 점수(logit)이므로, argmax를 이용해 가장 높은 점수를 받은 클래스를 예측값으로 선택합니다.
    y_pred_test = model(X_test)
    _, predicted = torch.max(y_pred_test, 1)

    # 정확도(Accuracy) 계산: 전체 예측 중 맞춘 예측의 비율
    correct = (predicted == y_test).sum().item()
    total = y_test.size(0)
    accuracy = 100 * correct / total
    print(f'\n평가 데이터 정확도: {accuracy:.2f}%')

# 새로운 붓꽃 데이터로 품종 예측해보기
# [꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비]
new_flower_data = np.array([[5.1, 3.5, 1.4, 0.2]], dtype=np.float32)
# 학습 때 사용한 scaler로 동일하게 스케일링을 해주어야 합니다.
new_flower_data_scaled = scaler.transform(new_flower_data)
new_flower_tensor = torch.from_numpy(new_flower_data_scaled)

with torch.no_grad():
    prediction_score = model(new_flower_tensor)
    predicted_class = torch.argmax(prediction_score, dim=1).item()
    print(f'\n새로운 붓꽃 데이터의 예측 품종: {iris.target_names[predicted_class]}')
```

#### **2. 해당 설명**

이 코드는 **회귀와 분류의 핵심적인 차이**를 명확하게 보여줍니다.
*   **모델의 출력:** 회귀 모델은 예측값으로 '하나의 연속적인 숫자'를 출력했지만(`nn.Linear(1, 1)`), 분류 모델은 '각 클래스별 점수(확률)'를 나타내는 '클래스 개수만큼의 숫자들'을 출력합니다(`nn.Linear(16, 3)`).
*   **손실 함수:** 회귀는 예측값과 정답 사이의 '거리'(오차)를 측정하는 **MSELoss**를 사용했습니다. 반면, 분류는 예측 확률 분포와 정답 확률 분포가 얼마나 다른지를 측정하는 **CrossEntropyLoss**를 사용합니다. 이는 분류 문제에 더 적합한 '틀린 정도'를 측정하는 척도입니다.
*   **평가 지표:** 회귀는 보통 손실 값 자체로 성능을 평가하지만, 분류는 **정확도(Accuracy)**, 정밀도(Precision), 재현율(Recall) 등 더 직관적인 지표로 성능을 평가합니다.

#### **3. 응용 가능한 예제**

**"고객의 이탈 여부 예측"**

고객의 '평균 구매액', '최근 접속일', '서비스 이용 기간' 등의 데이터를 입력받아, 해당 고객이 '이탈할 것(1)'인지 '유지될 것(0)'인지를 예측하는 이진 분류(Binary Classification) 모델을 만들 수 있습니다. 이 경우 모델의 최종 출력은 2개가 됩니다.

#### **4. 추가하고 싶은 내용 (원-핫 인코딩)**

텍스트에서 언급된 **원-핫 인코딩**은 `CrossEntropyLoss`를 사용할 때는 직접 해줄 필요가 없습니다. `y_train`에 `[0, 1, 2, ...]` 와 같은 클래스 인덱스를 그대로 넣어주면, `CrossEntropyLoss`가 내부적으로 알아서 원-핫 벡터처럼 계산을 처리해주기 때문입니다. 하지만 다른 손실 함수를 사용하거나 모델 구조에 따라서는 직접 원-핫 인코딩이 필요한 경우도 있습니다.

#### **5. 심화 내용 (데이터 전처리의 중요성 - `StandardScaler`)**

분류 예제에서는 **`StandardScaler`**를 사용해 데이터의 스케일을 맞추는 **표준화** 과정을 추가했습니다. 각 특성(꽃잎 길이, 꽃받침 너비 등)은 측정 단위와 범위가 모두 다릅니다. 이 상태로 학습하면, 범위가 큰 특성(예: 0~100)이 범위가 작은 특성(예: 0~1)보다 모델에 더 큰 영향을 미치게 되어 학습이 불안정해질 수 있습니다. 표준화는 모든 특성의 평균을 0, 표준편차를 1로 만들어, 모델이 모든 특성을 공평하게 고려하고 더 빠르고 안정적으로 학습하도록 돕는 매우 중요한 전처리 과정입니다.