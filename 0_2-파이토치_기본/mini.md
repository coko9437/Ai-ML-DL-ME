
---

### **코드 종합 설명**

이 코드는 PyTorch 라이브러리를 사용하여 **텐서(Tensor)**를 다루는 가장 기본적인 연산들을 실습합니다. 텐서는 딥러닝 모델의 입력, 출력, 그리고 모든 내부 계산에 사용되는 핵심 데이터 구조입니다. 이 예제들은 다음과 같은 필수적인 텐서 조작 기술들을 다룹니다.

1.  **텐서 생성 및 탐색:** `torch.rand`, `torch.argmax` 등을 이용해 텐서를 만들고 특정 조건에 맞는 값을 찾습니다.
2.  **차원 변경:** `squeeze`, `unsqueeze`를 사용해 텐서의 모양(shape)을 유연하게 변경합니다.
3.  **텐서 결합:** `cat`, `stack`을 이용해 여러 텐서를 하나로 합치는 방법과 그 차이점을 배웁니다.
4.  **인덱싱 및 슬라이싱:** `index_select`, `topk`, `masked_fill` 등을 활용하여 텐서의 특정 부분만 추출하거나 값을 변경합니다.
5.  **분할 및 섞기:** `randperm`, `chunk`, `split`을 통해 데이터셋을 무작위로 섞거나 여러 개의 작은 묶음(배치)으로 나눕니다.
6.  **확장:** `expand`를 사용해 텐서의 데이터를 복사하지 않고 크기를 늘리는 효율적인 방법을 익힙니다.

이러한 기본 연산들은 복잡한 딥러닝 모델을 구현하고, 데이터를 모델에 입력하기 적절한 형태로 가공(전처리)하는 데 반드시 필요한 기초 기술입니다.

---

### **Part 1: 텐서 생성, 탐색 및 차원 조작 (문제 1, 2)**

텐서를 만들고, 원하는 정보를 추출하며, 필요에 따라 형태를 바꾸는 가장 기본적인 작업입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
import torch

# 📌 문제 1: PyTorch에서 랜덤 텐서를 생성하고, 가장 큰 값을 가진 인덱스를 찾으세요.

# (1) 4x4 크기의, 0과 1 사이의 균등 분포를 따르는 랜덤 텐서를 생성합니다.
# torch.rand(행, 열)
x = torch.rand(4, 4)
print("랜덤 텐서 x:\n", x)

# (2) 각 행(dim=1)에서 가장 큰 값을 가진 원소의 인덱스(위치)를 찾습니다.
# torch.argmax(입력_텐서, dim=차원)
# dim=0은 열 기준, dim=1은 행 기준입니다.
# 4개의 행에 대해 각각 최대값의 인덱스를 반환하므로, 결과는 4개의 원소를 가진 1차원 텐서가 됩니다.
max_indices = torch.argmax(x, dim=1)
print("각 행의 최대값 인덱스:", max_indices)

# 📌 문제 2: squeeze()와 unsqueeze()를 이용해 텐서의 차원을 조작하세요.

# (1) (1, 3, 1, 4) 크기(shape)의 4차원 랜덤 텐서를 생성합니다.
x = torch.rand(1, 3, 1, 4)
print("원본 텐서의 shape:", x.shape) # 결과: torch.Size([1, 3, 1, 4])

# (2) squeeze()를 사용하여 크기가 1인 차원을 모두 제거합니다.
# (1, 3, 1, 4) -> (3, 4)
# 0번째 차원과 2번째 차원의 크기가 1이므로 제거됩니다.
x_squeezed = x.squeeze()
print("squeeze() 후 shape:", x_squeezed.shape) # 결과: torch.Size([3, 4])

# (3) unsqueeze()를 사용하여 특정 위치에 크기 1인 새 차원을 추가합니다.
# (3, 4) -> (3, 1, 4)
# unsqueeze(dim)은 지정된 dim 위치에 새로운 차원을 삽입합니다.
# 여기서는 1번 인덱스 위치(두 번째 차원)에 차원을 추가합니다.
# _(언더스코어)가 붙은 함수(unsqueeze_)는 'in-place' 연산으로,
# 새로운 텐서를 반환하는 대신 원본 텐서 자체를 변경합니다.
x_squeezed.unsqueeze_(1)
print("unsqueeze_() 후 shape:", x_squeezed.shape) # 결과: torch.Size([3, 1, 4])```

#### **2. 해당 설명**

**`argmax`**는 모델의 예측 결과를 해석할 때 매우 중요하게 사용됩니다. 
예를 들어, 10개의 클래스(0~9)를 분류하는 모델이 이미지에 대해 `[0.1, 0.05, 0.8, ...]` 와
같은 확률 값을 출력했을 때, `argmax`를 사용하면 가장 확률이 높은(즉, 모델이 예측한) 클래스의
인덱스(여기서는 2)를 쉽게 찾을 수 있습니다.

**`squeeze`**와 **`unsqueeze`**는 모델의 입력과 출력 형태를 맞추는 데 필수적입니다.
딥러닝 모델은 정해진 차원의 입력만 받기 때문에, 데이터의 차원이 맞지 않을 때 이 함수들을
이용해 형태를 바꿔줘야 합니다. 예를 들어, (높이, 너비) 형태의 흑백 이미지를 모델에 
입력하려면, 채널 차원을 추가하여 (1, 높이, 너비) 또는 (높이, 너비, 1) 형태로 만들어주어야 합니다.

#### **3. 응용 가능한 예제**

**"이미지 분류 모델의 예측 결과 확인"**

3개의 이미지에 대해 5개의 클래스 중 하나로 분류하는 모델의 출력(예측 확률 텐서)이 
`predictions = torch.rand(3, 5)` 라고 할 때, `torch.argmax(predictions, dim=1)`을
실행하면 3개의 각 이미지에 대한 최종 예측 클래스(0~4)를 얻을 수 있습니다.

#### **4. 추가하고 싶은 내용 (`keepdim` 옵션)**

`torch.max`나 `torch.sum` 같은 연산을 할 때 `keepdim=True` 옵션을 사용하면, 
연산이 이루어진 차원이 크기 1로 유지되어 원래 텐서의 차원 수가 보존됩니다. 
이는 나중에 브로드캐스팅(Broadcasting) 연산을 할 때 매우 유용합니다.

```python
values, indices = torch.max(x, dim=1, keepdim=True)
print(values.shape) # 결과: torch.Size([4, 1]) (원래는 torch.Size([4]))
```

#### **5. 심화 내용 (`squeeze`와 `unsqueeze`의 중요성)**

최신 딥러닝 모델(특히 Transformer)에서는 행렬 곱셈을 위해 텐서의 차원을 매우 동적으로 바꿔야 하는 경우가 많습니다. 예를 들어, 어텐션 스코어를 계산할 때 특정 차원을 추가(`unsqueeze`)하여 브로드캐스팅이 가능하게 만들고, 계산이 끝난 후에는 다시 원래 형태로 돌리기 위해 `squeeze`를 사용하는 패턴이 빈번하게 등장합니다. 이 두 함수에 익숙해지는 것은 복잡한 모델 코드를 이해하는 첫걸음입니다.

---

### **Part 2: 텐서 결합 및 인덱싱 (문제 3, 4)**

여러 개의 텐서를 합치거나, 텐서의 특정 부분만을 골라내는 기술입니다. 데이터 배치를 만들거나 특정 데이터를 추출할 때 사용됩니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
# 📌 문제 3: torch.cat()과 torch.stack()의 차이를 확인하세요.
# (1) 크기가 (2,3)인 두 개의 텐서를 만듭니다.
a = torch.tensor([[1, 2, 3], [4, 5, 6]])      # Shape: (2, 3)
b = torch.tensor([[7, 8, 9], [10, 11, 12]]) # Shape: (2, 3)

# (2) torch.cat()을 이용하여 dim=0(행) 방향으로 두 텐서를 이어 붙입니다.
# cat은 'concatenate'(연결)의 약자입니다.
# 기존 차원을 유지하면서 해당 차원을 따라 데이터를 이어 붙입니다.
# (2, 3)과 (2, 3)을 행 방향으로 붙이면 -> (4, 3)
cat_result = torch.cat([a, b], dim=0)
print("cat() 결과 (dim=0):\n", cat_result)
print("cat() 결과 shape:", cat_result.shape) # 결과: torch.Size([4, 3])

# (3) torch.stack()을 이용하여 dim=0 방향으로 두 텐서를 쌓습니다.
# stack은 새로운 차원을 만들어 그 차원에 데이터를 쌓습니다.
# (2, 3) 텐서 두 개를 dim=0 방향으로 쌓으면 -> (2, 2, 3)
# 0번째 차원이 새로 생기고, 그 차원에 a와 b가 각각 들어갑니다.
stack_result = torch.stack([a, b], dim=0)
print("stack() 결과 (dim=0):\n", stack_result)
print("stack() 결과 shape:", stack_result.shape) # 결과: torch.Size([2, 2, 3])

# 📌 문제 4: torch.index_select()를 이용해 특정 행/열을 선택하세요.
# (1) 크기가 (4,4)인 랜덤 텐서를 생성합니다.
x = torch.rand(4, 4)
print("원본 텐서:\n", x)

# (2) 첫 번째 열(인덱스 0)과 세 번째 열(인덱스 2)을 선택하여 새로운 텐서를 만듭니다.
# torch.index_select(입력_텐서, dim=차원, index=선택할_인덱스_텐서)
# dim=1 이므로 열을 기준으로 선택합니다.
indices = torch.tensor([0, 2])
selected_cols = torch.index_select(x, dim=1, index=indices)
print("선택된 열들:\n", selected_cols)
```

#### **2. 해당 설명**

**`cat`**과 **`stack`**의 차이를 이해하는 것은 매우 중요합니다. `cat`은 여러 데이터를 하나의 긴 데이터로 이어 붙일 때 사용됩니다. 예를 들어, 여러 파일에 나뉘어 저장된 데이터를 불러와 하나의 큰 데이터셋으로 만들 때 사용할 수 있습니다.

반면 **`stack`**은 여러 개의 개별 데이터를 하나의 배치(batch)로 묶을 때 주로 사용됩니다. 예를 들어, (3, 224, 224) 크기의 이미지 32개를 불러와 `stack`을 사용하면, (32, 3, 224, 224) 크기의 배치 텐서를 만들 수 있습니다. 여기서 32는 배치 사이즈(batch size)를 나타내는 새로운 차원이 됩니다.

**`index_select`**는 순서에 상관없이 내가 원하는 위치의 데이터를 골라낼 때 사용합니다. 일반적인 슬라이싱(`x[:, [0, 2]]`)으로도 비슷한 작업이 가능하지만, `index_select`는 GPU 연산에서 더 효율적일 수 있으며, 코드를 더 명시적으로 만들어줍니다.

#### **3. 응용 가능한 예제**

**"특정 클래스의 데이터만 추출하기"**

어떤 데이터셋의 레이블(정답)이 `labels = torch.tensor([0, 1, 1, 0, 2])` 일 때, `indices = (labels == 1).nonzero().squeeze()` 코드를 이용해 레이블이 1인 데이터의 인덱스(`[1, 2]`)를 찾을 수 있습니다. 그 후 `torch.index_select(data, dim=0, index=indices)`를 실행하면 전체 데이터에서 레이블이 1인 데이터만 골라낼 수 있습니다.

#### **4. 추가하고 싶은 내용 (`gather` 함수)**

`index_select`가 행이나 열 전체를 가져오는 것이라면, `torch.gather`는 각 행(또는 열)마다 내가 원하는 위치의 '단일 원소'를 하나씩 가져올 수 있는 더 유연한 함수입니다. 복잡한 인덱싱이 필요할 때 사용됩니다.

#### **5. 심화 내용 (`stack` vs `cat`의 의미)**

*   `cat` -> "데이터의 양을 늘린다." (예: 100개 데이터와 50개 데이터를 합쳐 150개 데이터로)
*   `stack` -> "데이터의 차원을 늘린다." (예: 100개의 1차원 벡터를 합쳐 100xN 크기의 2차원 행렬로)

이 개념적 차이를 이해하면 언제 어떤 함수를 써야 할지 명확해집니다.

---

### **Part 3: 텐서 마스킹 및 분할 (문제 5, 6)**

텐서의 특정 조건에 맞는 값을 변경하거나, 전체 데이터를 여러 조각으로 나누는 기술입니다. 데이터 증강(augmentation), 미니배치(mini-batch) 생성 등에 활용됩니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
# 📌 문제 5: torch.topk()와 masked_fill()을 조합하여 특정 조건을 만족하는 값을 변경하세요.
# (이 예제는 topk와 scatter_를 사용했으므로, 그 기준으로 설명합니다.)

# (1) 크기가 (3,5)인 랜덤 텐서를 생성합니다.
x = torch.rand(3, 5)
print("원본 텐서:\n", x)

# (2) 각 행(dim=1)에서 상위 3개(k=3)의 값과 그 인덱스를 찾습니다.
# topk는 가장 큰 k개의 원소와 그 인덱스를 반환합니다.
values, indices = torch.topk(x, k=3, dim=1)
print("상위 3개 값:", values)
print("상위 3개 값의 인덱스:", indices)

# (3) 상위 3개의 위치에만 1을 표시하는 마스크(mask) 텐서를 만듭니다.
# torch.zeros_like(x) : x와 동일한 크기의 0으로 채워진 텐서를 생성
# .scatter_(dim, index, src) : 특정 인덱스 위치에 값을 뿌려주는(scatter) 함수
# dim=1(열 기준)으로, indices 위치에, 1이라는 값을 채워 넣습니다.
# _가 붙어있어 원본 텐서(zeros_like)가 직접 변경됩니다.
mask = torch.zeros_like(x).scatter_(1, indices, 1)
print("마스크:\n", mask)

# (4) 원본 텐서 x와 마스크를 원소별로 곱하여, 상위 3개 값만 남기고 나머지는 0으로 만듭니다.
masked_x = x * mask
print("상위 3개 값만 유지된 텐서:\n", masked_x)

# 📌 문제 6: torch.randperm()을 이용하여 데이터를 랜덤하게 섞은 후, torch.chunk()로 나누세요.

# (1) 0부터 9까지 10개의 원소를 가진 1차원 텐서를 생성합니다.
x = torch.arange(10)
print("원본 텐서:", x)

# (2) torch.randperm(n)을 사용하여 0부터 n-1까지의 정수를 무작위 순서로 나열한 텐서를 생성합니다.
# 이 인덱스를 이용해 원본 텐서 x를 섞습니다. 이를 '셔플링(shuffling)'이라고 합니다.
shuffled_indices = torch.randperm(10)
shuffled_x = x[shuffled_indices]
print("섞인 텐서:", shuffled_x)

# (3) torch.chunk(입력_텐서, chunks=개수)를 사용하여 텐서를 N개의 조각으로 나눕니다.
# 10개의 원소를 2개의 조각으로 나누므로, 각 조각은 5개의 원소를 갖게 됩니다.
chunks = torch.chunk(shuffled_x, chunks=2)
print("첫 번째 부분:", chunks[0])
print("두 번째 부분:", chunks[1])
```

#### **2. 해당 설명**

**마스킹(Masking)**은 딥러닝에서 매우 중요한 기법입니다. 문제 5와 같이 특정 값만 남기거나, 문제 9처럼 특정 조건에 맞는 값을 다른 값으로 바꿀 때 사용됩니다. 특히 자연어 처리에서 문장의 길이를 맞추기 위해 추가된 불필요한 패딩(padding) 토큰에 대해 어텐션 계산을 하지 않도록 마스킹하는 것이 대표적인 예입니다.

**`randperm`**과 **`chunk`**는 훈련(Training) 데이터를 준비하는 과정의 핵심입니다. 모델을 학습시킬 때, 전체 데이터를 한 번에 메모리에 올리고 계산하는 것은 비효율적입니다. 따라서 데이터를 무작위로 섞어(`randperm`으로 셔플링) 데이터의 순서에 대한 편향을 없앤 뒤, **미니배치(mini-batch)**라는 작은 묶음으로 나누어(`chunk`) 학습을 진행합니다. 이 과정을 통해 안정적이고 효율적인 모델 학습이 가능해집니다.

#### **3. 응용 가능한 예제**

**"Dropout 구현 맛보기"**

Dropout은 학습 과정에서 일부 뉴런을 무작위로 0으로 만들어 과적합을 방지하는 기법입니다. `mask = (torch.rand(shape) > p) * (1/(1-p))` 와 같은 코드로 무작위 마스크를 생성한 뒤, `layer_output * mask` 처럼 곱해주는 방식으로 간단하게 구현의 핵심 원리를 체험해 볼 수 있습니다.

#### **4. 추가하고 싶은 내용 (scatter_의 활용)**

`scatter_` 함수는 원-핫 인코딩(One-hot encoding) 벡터를 만들 때 매우 유용합니다. 원-핫 인코딩은 정답 레이블을 벡터 형태로 표현하는 방식으로, `torch.zeros(num_samples, num_classes).scatter_(1, labels.unsqueeze(1), 1)` 와 같은 코드로 한 번에 생성할 수 있습니다.

#### **5. 심화 내용 (DataLoader)**

실제 프로젝트에서는 `randperm`이나 `chunk`를 직접 사용하는 경우는 드뭅니다. PyTorch는 `torch.utils.data.DataLoader`라는 강력한 유틸리티를 제공하는데, 이 클래스는 데이터셋 셔플링, 미니배치 생성, 병렬 데이터 로딩 등의 모든 과정을 자동화하여 훨씬 편리하고 효율적인 데이터 준비를 가능하게 해줍니다.

---

### **Part 4: 텐서 확장 및 고급 분할 (문제 7, 8, 9, 10)**

메모리를 효율적으로 사용하며 텐서의 크기를 조작하고, 다양한 조건에 따라 텐서를 나누거나 값을 변경하는 고급 기술입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
# 📌 문제 7: expand()를 사용하여 텐서 크기를 확장하세요.

# (1) 크기가 (1, 3)인 텐서를 생성합니다.
x = torch.tensor([[1, 2, 3]])  # Shape: (1, 3)
print("원본 텐서:\n", x)

# (2) expand()를 사용하여 (5, 3) 크기로 확장합니다.
# expand는 새로운 메모리를 할당하여 데이터를 복사하는 것이 아니라,
# 기존 데이터를 가리키는 뷰(view)만 생성하여 크기를 확장합니다.
# 크기가 1인 차원만 더 큰 숫자로 확장할 수 있습니다.
# 0번째 차원(행)이 1에서 5로 확장됩니다.
expanded_x = x.expand(5, 3)
print("expand() 적용 후:\n", expanded_x)
# [[1, 2, 3],
#  [1, 2, 3],
#  [1, 2, 3],
#  [1, 2, 3],
#  [1, 2, 3]]

# 📌 문제 8: split()과 chunk()의 차이를 이해하세요.

# (1) 크기가 (10,)인 1D 텐서를 생성합니다.
x = torch.arange(10)
print("원본 텐서:", x)

# (2) split()을 사용하여 크기를 직접 지정하여 텐서를 나눕니다.
# torch.split(입력_텐서, 분할_크기_리스트)
# [3, 3, 4]는 각각 3개, 3개, 4개의 원소를 갖는 3개의 텐서로 나누라는 의미입니다.
split_result = torch.split(x, [3, 3, 4])
print("split() 결과:", split_result) # (tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8, 9]))

# (3) chunk()을 사용하여 개수를 지정하여 텐서를 나눕니다.
# 10개의 원소를 3개의 덩어리(chunk)로 최대한 균등하게 나눕니다.
# 10 / 3 = 3.33... 이므로, 4개, 3개, 3개 또는 4, 4, 2 등 최대한 비슷하게 나눕니다. (여기서는 4, 3, 3)
chunk_result = torch.chunk(x, chunks=3)
print("chunk() 결과:", chunk_result) # (tensor([0, 1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))

# 📌 문제 9: masked_fill()을 사용하여 특정 조건을 만족하는 요소를 변경하세요.

# (1) 크기가 (3, 3)인 랜덤 텐서를 생성합니다.
x = torch.rand(3, 3)
print("원본 텐서:\n", x)

# (2) 조건(x < 0.5)을 만족하는(True) 위치의 원소를 -1로 변경합니다.
# .masked_fill(마스크_조건, 변경할_값)
# 마스크_조건 텐서에서 True인 위치의 원본 텐서 값들이 -1로 채워집니다.
masked_x = x.masked_fill(x < 0.5, -1)
print("masked_fill() 적용 후:\n", masked_x)

# 📌 문제 10: torch.index_select()를 이용하여 특정 행을 선택하세요.
# (이 문제는 문제 4와 유사하며, dim만 다릅니다.)

# (1) 크기가 (5, 4)인 랜덤 텐서를 생성합니다.
x = torch.rand(5, 4)
print("원본 텐서:\n", x)

# (2) 두 번째 행(인덱스 1)과 네 번째 행(인덱스 3)만 선택합니다.
# dim=0 이므로 행을 기준으로 선택합니다.
indices = torch.tensor([1, 3])
selected_rows = torch.index_select(x, dim=0, index=indices)
print("선택된 행들:\n", selected_rows)```

#### **2. 해당 설명**

**`expand`**는 **브로드캐스팅(Broadcasting)**이라는 개념과 깊은 관련이 있습니다. 브로드캐스팅은 PyTorch가 서로 다른 크기의 텐서 간 연산을 수행할 때, 자동으로 크기를 맞춰주는 기능입니다. `expand`는 이 브로드캐스팅이 일어나는 방식을 명시적으로 보여주는 함수로, 메모리를 매우 효율적으로 사용하며 연산할 수 있게 해줍니다. (예: `(5, 3) 텐서 + (1, 3) 텐서` 연산 시, (1, 3) 텐서가 내부적으로 `expand` 되어 (5, 3)처럼 동작합니다.)

**`split`**과 **`chunk`**의 차이는 명확합니다. `split`은 내가 원하는 크기로 정확하게 자를 때 사용하고(예: 훈련/검증/테스트 데이터셋을 7:2:1 비율로 나눌 때), `chunk`는 크기는 상관없이 동일한 개수로 나눌 때 사용합니다(예: 미니배치 생성).

**`masked_fill`**은 특정 조건을 만족하는 값을 일괄적으로 바꿀 때 매우 유용한 함수입니다. 예를 들어, 계산 과정에서 `inf`(무한대)나 `NaN`(Not a Number) 값이 발생했을 때, 이를 0이나 다른 안정적인 값으로 대체하여 모델의 학습 안정성을 높이는 데 사용될 수 있습니다.

#### **3. 응용 가능한 예제**

**"텐서에 편향(bias) 더하기"**

(32, 10) 크기의 모델 출력(`output`)에 (10,) 크기의 편향(`bias`) 텐서를 더할 때, `bias` 텐서는 내부적으로 `bias.expand(32, 10)` 과 같이 동작하여 32개의 모든 샘플에 동일한 편향을 더해줍니다. 이것이 브로드캐스팅의 대표적인 예입니다.

#### **4. 추가하고 싶은 내용 (`where` 함수)**

`masked_fill`은 조건을 만족하는 곳을 특정 '단일 값'으로 채웁니다. 반면 `torch.where(condition, x, y)` 함수는 `condition`이 True인 곳은 `x` 텐서의 값으로, False인 곳은 `y` 텐서의 값으로 채워 넣어, 훨씬 더 유연하게 값을 변경할 수 있습니다.

#### **5. 심화 내용 (`expand` vs `repeat`)**

`expand`와 비슷한 함수로 `repeat`이 있습니다. `x.repeat(5, 1)`도 (1, 3) 텐서를 (5, 3)으로 만들지만, 결정적인 차이가 있습니다.
*   **`expand`**: 메모리를 공유하는 뷰(View)를 생성합니다. 매우 빠르고 메모리 효율적입니다. 확장된 텐서의 한 값을 바꾸면 원본도 바뀔 수 있습니다.
*   **`repeat`**: 데이터를 실제로 메모리에 복사하여 새로운 텐서를 생성합니다. 메모리 사용량이 크고 상대적으로 느립니다.

대부분의 경우 `expand`나 브로드캐스팅을 사용하는 것이 훨씬 효율적이며, 데이터 자체를 복제해야 하는 특별한 경우에만 `repeat`을 사용하는 것이 좋습니다.