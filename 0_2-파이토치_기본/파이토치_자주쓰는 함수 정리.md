
---

### **코드 종합 설명**

이 코드는 PyTorch에서 딥러닝의 기본 재료인 **텐서(Tensor)**를 생성, 조작, 변환하는 거의 모든 핵심 함수를 다루는 종합 실습서입니다. 전체적인 흐름은 다음과 같습니다.

1.  **텐서 생성:** `torch.rand`, `torch.randn`, `torch.ones` 등 다양한 분포와 방식으로 텐서를 생성하는 방법을 배웁니다. 이는 모델의 초기 가중치를 설정하거나 가짜 데이터를 만드는 데 필수적입니다.
2.  **텐서 형태 변환 (Shape Manipulation):** 텐서의 차원과 모양을 자유자재로 바꾸는 `view`, `squeeze`, `unsqueeze`, `permute` 등의 함수를 학습합니다. 이는 모델의 각 계층(layer)에 맞는 입력 형태로 데이터를 가공하는 데 핵심적인 역할을 합니다.
3.  **텐서 결합 및 분할:** 여러 텐서를 하나로 합치거나(`cat`, `stack`), 하나의 큰 텐서를 여러 개의 작은 텐서로 나누는(`split`, `chunk`) 방법을 배웁니다. 데이터셋을 구성하거나 미니배치(mini-batch)를 만드는 데 사용됩니다.
4.  **텐서 인덱싱 및 조건부 조작:** `index_select`, `argmax`, `topk`, `masked_fill`과 같은 함수를 통해 텐서의 특정 원소를 추출하거나, 특정 조건을 만족하는 원소의 값을 변경하는 고급 기술을 익힙니다.
5.  **메모리 관리:** 인플레이스(In-place) 연산의 개념과 `expand` 함수의 효율적인 메모리 사용법을 이해하여, 대규모 텐서를 다룰 때 성능을 최적화하는 방법을 배웁니다.

이 함수들은 복잡한 딥러닝 모델을 구축하고 데이터를 자유자재로 다루기 위한 **'기본 도구 상자'** 와 같으며, 각각의 미묘한 차이점을 이해하는 것이 매우 중요합니다.

---

### **Part 1: 텐서 생성 및 기본 속성**

모델의 가중치, 입력 데이터 등 모든 것의 시작점인 텐서를 다양한 방식으로 생성하는 방법을 배웁니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
import torch

# 📌 2-0: 다양한 랜덤 텐서 생성
# torch.randn: 평균 0, 표준편차 1의 표준 정규 분포에서 샘플링 (가장 일반적인 가중치 초기화 방식)
tensor_randn = torch.randn(2, 3)

# torch.rand: 0과 1 사이의 균등 분포에서 샘플링
tensor_rand = torch.rand(2, 3)

# torch.randint(low, high, size): low(포함)와 high(미포함) 사이의 정수를 무작위로 샘플링
tensor_randint = torch.randint(0, 10, (2, 3))

# torch.randperm(n): 0부터 n-1까지의 정수를 무작위로 섞어 순열(permutation)을 생성
tensor_perm = torch.randperm(5)


# 📌 2-14: 특정 값으로 채워진 텐서 생성
# torch.ones(size): 지정한 크기만큼 1로 채워진 텐서 생성
tensor_ones = torch.ones(2, 3)

# torch.zeros_like(tensor): 다른 텐서와 동일한 크기(shape)의 0으로 채워진 텐서 생성
tensor_zeros_like = torch.zeros_like(tensor_randn)


# 📌 2-1: 인플레이스(In-place) 연산
# 인플레이스 연산은 새로운 메모리를 할당하지 않고, 원본 텐서의 값을 직접 수정합니다. 함수명 끝에 _(언더스코어)가 붙습니다.
x_original = torch.ones(3)
print("원본 x:", x_original)
# 일반 연산: 새로운 텐서 y를 생성하고, x는 변하지 않음
y = x_original + 2
print("일반 연산 후 x:", x_original) # [1., 1., 1.]
print("새로운 텐서 y:", y)      # [3., 3., 3.]

# 인플레이스 연산: x_original의 값이 직접 변경됨
x_original.add_(2)
print("인플레이스 연산 후 x:", x_original) # [3., 3., 3.]
```

#### **2. 해당 설명**

텐서 생성은 딥러닝의 출발점입니다. **`randn`**은 모델의 가중치를 처음에 무작위로 설정(초기화)할 때 가장 널리 사용되며, 이는 모델이 특정 방향에 치우치지 않고 학습을 시작하게 돕습니다. **`randperm`**은 데이터셋의 순서를 무작위로 섞어(shuffling) 모델이 데이터 순서에 과적합되는 것을 방지하는 데 필수적입니다.

**인플레이스 연산**은 메모리 효율성을 높일 수 있는 방법입니다. 거대한 텐서를 다룰 때, 매 연산마다 새로운 텐서를 생성하는 것은 상당한 메모리 부담을 줄 수 있습니다. 이럴 때 인플레이스 연산을 사용하면 메모리 사용량을 줄일 수 있습니다. 하지만, 자동 미분(Autograd) 과정에서 기울기 계산에 필요한 중간 값을 덮어쓸 수 있어, **모델 학습 중에는 사용을 피하는 것이 일반적**이며, 주로 추론(inference) 단계나 특정 데이터 전처리 과정에서 제한적으로 사용됩니다.

#### **3. 응용 가능한 예제**

**"모델 가중치 초기화"**

`nn.Linear`와 같은 PyTorch 레이어는 내부적으로 `torch.randn`과 유사한 방법으로 가중치를 초기화합니다. 때로는 Xavier 초기화나 He 초기화 같은 특정 초기화 기법을 직접 구현해야 할 때, `torch.normal`이나 `torch.rand` 같은 함수들을 조합하여 사용하게 됩니다.

#### **4. 추가하고 싶은 내용 (시드 고정)**

실험의 **재현 가능성(reproducibility)**을 위해 랜덤 함수의 결과를 고정해야 할 때가 있습니다. `torch.manual_seed(42)`와 같이 코드를 시작할 때 시드(seed)를 고정하면, `randn`, `rand` 등 모든 랜덤 함수의 결과가 항상 동일하게 나옵니다. 이는 다른 사람과 코드를 공유하거나, 특정 실험 결과를 다시 확인해야 할 때 매우 중요합니다.

#### **5. 심화 내용 (메모리 레이아웃과 `contiguous`)**

`view()` 함수를 설명하기 전에 알아야 할 개념입니다. 텐서는 기본적으로 메모리에 **연속적인(contiguous)** 형태로 저장됩니다. 하지만 `transpose()`나 특정 슬라이싱 연산은 실제 데이터를 재배열하는 대신, 데이터를 가리키는 방식(stride)만 바꾸어 비연속적인(non-contiguous) 뷰(view)를 만듭니다. `view()` 함수는 연속적인 메모리에서만 동작하므로, 비연속적인 텐서에 `view()`를 사용하려면 `.contiguous()`를 호출하여 메모리를 재배열해주어야 합니다.

---

### **Part 2: 텐서 형태 변환 (Shape Manipulation)**

데이터를 모델의 각 계층이 요구하는 형태로 '빚어내는' 과정입니다. 딥러닝 코드에서 가장 빈번하게 사용되는 연산들입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
# 📌 2-2: view() - 텐서의 모양을 변경 (메모리 공유)
# view()는 원소의 개수를 유지하면서 텐서의 shape을 변경합니다. 원본 텐서와 메모리를 공유합니다.
x = torch.arange(12) # 1차원 텐서 [0, 1, ..., 11], Shape: (12,)
# 3행 4열의 2차원 텐서로 변경. -1을 사용하면 나머지 차원으로부터 크기를 자동으로 계산합니다.
x_view = x.view(3, -1) # -1은 자동으로 4로 계산됨
print("view() 결과 shape:", x_view.shape) # 결과: torch.Size([3, 4])
print("view() 결과:\n", x_view)

# view()는 메모리를 공유하므로, view로 만든 텐서의 값을 바꾸면 원본 텐서도 바뀝니다.
x_view[0, 0] = 99
print("변경된 view:\n", x_view)
print("영향 받은 원본:", x) # x의 첫 번째 원소도 99로 바뀜


# 📌 2-3: squeeze() vs unsqueeze() - 차원 추가 및 제거
# unsqueeze(dim): 지정된 위치(dim)에 크기가 1인 차원을 추가합니다.
y = torch.tensor([1, 2, 3]) # Shape: (3,)
# 0번 위치에 배치 차원을 추가하여 딥러닝 모델의 단일 입력 형태로 만듭니다.
y_unsqueezed = y.unsqueeze(0) # (3,) -> (1, 3)
print("\nunsqueeze(0) 결과 shape:", y_unsqueezed.shape)

# squeeze(): 크기가 1인 차원을 모두 제거합니다.
z = torch.randn(1, 3, 1, 4) # Shape: (1, 3, 1, 4)
z_squeezed = z.squeeze()    # (1, 3, 1, 4) -> (3, 4)
print("squeeze() 결과 shape:", z_squeezed.shape)


# 📌 2-4-4: 3차원 텐서 인덱싱 이해
# Shape: (Batch, Height, Width) = (2, 3, 4)
tensor_3d = torch.tensor([
    [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],    # 첫 번째 배치 (인덱스 0)
    [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]] # 두 번째 배치 (인덱스 1)
])
# [배치 인덱스, 높이(행) 인덱스, 너비(열) 인덱스]
print("\n3D 텐서의 [1, 0, 3] 위치 값:", tensor_3d[1, 0, 3]) # 두 번째 배치의 첫 번째 행, 네 번째 열 -> 16

# 모든 배치에서, 모든 행의, 2번 인덱스 열만 선택
print("모든 배치의 3번째 열:\n", tensor_3d[:, :, 2])
```

#### **2. 해당 설명**

**`view()`**는 딥러닝에서 가장 많이 사용되는 함수 중 하나입니다. 예를 들어, CNN을 통과한 이미지 특성 맵(feature map)은 보통 (배치크기, 채널, 높이, 너비) 형태의 4D 텐서인데, 이를 완전 연결 계층(Fully Connected Layer)에 입력하기 위해서는 (배치크기, 채널\*높이\*너비) 형태의 2D 텐서로 '펼쳐주는' 작업이 필요합니다. 이때 `output.view(batch_size, -1)`와 같은 코드가 사용됩니다. `view()`는 데이터를 복사하지 않고 모양만 바꾸므로 매우 빠르고 효율적입니다.

**`unsqueeze`**는 주로 **배치 차원(batch dimension)**을 추가할 때 사용됩니다. 모델은 보통 여러 개의 데이터를 한 묶음(배치)으로 처리하도록 설계되므로, 단 하나의 데이터 샘플을 모델에 입력하려 할 때 `unsqueeze(0)`를 사용해 `(특성, ...)` 형태를 `(1, 특성, ...)` 형태로 만들어주어야 합니다. 반대로 모델의 출력이 `(1, 클래스개수)`와 같이 불필요한 배치 차원을 포함할 때 **`squeeze`**를 사용해 `(클래스개수)` 형태로 바꿔주면 후속 처리가 편리해집니다.

#### **3. 응용 가능한 예제**

**"이미지 데이터를 모델 입력 형태로 변환하기"**

28x28 크기의 흑백 이미지 데이터가 `(28, 28)` 형태의 텐서로 로드되었을 때, 이를 CNN 모델에 입력하기 위해서는 `(배치크기, 채널수, 높이, 너비)` 형태가 필요합니다. `image.unsqueeze(0).unsqueeze(0)` 코드를 사용하면 `(28, 28)` -> `(1, 28, 28)` -> `(1, 1, 28, 28)` 형태로 변환하여 모델에 입력할 수 있습니다.

#### **4. 추가하고 싶은 내용 (`reshape` vs `view`)**

`reshape()`은 `view()`와 거의 동일한 기능을 하지만 더 유연합니다. `view()`는 메모리가 연속적일 때만 사용 가능하지만, `reshape()`은 메모리가 비연속적일 경우 자동으로 `.contiguous()`를 호출하여 메모리를 재배열한 후 모양을 바꿔줍니다. 따라서 더 안전하지만, 의도치 않은 데이터 복사가 발생하여 성능이 저하될 수 있습니다. 대부분의 경우 `view()`를 사용하고, 메모리 연속성 에러가 발생할 때 `reshape()`이나 `.contiguous().view()`를 사용하는 것이 좋습니다.

#### **5. 심화 내용 (Shape 분석의 중요성)**

제공해주신 '2-4-3' 텍스트처럼, 모델의 각 계층을 데이터가 통과할 때마다 **Shape이 어떻게 변하는지 추적하는 능력**은 딥러닝 모델을 디버깅하고 이해하는 데 가장 중요한 기술 중 하나입니다. 복잡한 모델을 다룰 때는 `print(x.shape)`를 코드 중간중간에 삽입하여 데이터의 흐름을 눈으로 확인하는 것이 매우 효과적입니다. Shape이 맞지 않아 발생하는 에러(Shape Mismatch Error)는 딥러닝에서 가장 흔하게 마주치는 에러입니다.

---

### **Part 3: 텐서 결합 및 분할**

데이터를 합치고 나누는 기술입니다. 여러 소스의 데이터를 통합하거나, 대용량 데이터를 학습에 적합한 작은 단위로 쪼갤 때 사용됩니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
# 📌 2-7: cat() - 기존 차원을 따라 텐서를 이어 붙임
a = torch.tensor([[1, 2], [3, 4]]) # Shape: (2, 2)
b = torch.tensor([[5, 6]])         # Shape: (1, 2)
# dim=0 (행) 방향으로 연결. 열의 개수(2)는 동일해야 합니다.
cat_result = torch.cat([a, b], dim=0)
print("cat() 결과 (dim=0):\n", cat_result)
print("cat() 결과 shape:", cat_result.shape) # (2, 2) + (1, 2) -> (3, 2)


# 📌 2-8: stack() - 새로운 차원을 만들어 텐서를 쌓음
x = torch.tensor([1, 2, 3]) # Shape: (3,)
y = torch.tensor([4, 5, 6]) # Shape: (3,)
# 두 텐서의 shape이 완전히 동일해야 합니다.
# dim=0 위치에 새로운 차원을 추가하여 쌓습니다.
stack_result = torch.stack([x, y], dim=0)
print("\nstack() 결과 (dim=0):\n", stack_result)
print("stack() 결과 shape:", stack_result.shape) # 2개의 (3,) 텐서 -> (2, 3)

# 📌 2-3-2: unsqueeze와 cat의 조합 (stack과 동일한 결과)
# stack은 사실 unsqueeze와 cat의 조합으로 구현될 수 있습니다.
# 각 텐서에 배치 차원을 추가(unsqueeze)한 뒤, 그 배치 차원을 따라 이어 붙이면(cat) stack과 동일합니다.
z = torch.cat([x.unsqueeze(0), y.unsqueeze(0)], dim=0)
print("\nunsqueeze+cat 결과 shape:", z.shape) # (1, 3) + (1, 3) -> (2, 3)


# 📌 2-5: split() vs chunk() - 텐서 나누기
data = torch.arange(10) # [0, 1, ..., 9]
# split(tensor, split_size_or_sections): '크기'를 기준으로 나눔
# 크기가 3인 덩어리로 나눔. 마지막 덩어리는 남은 원소(1개)를 가짐.
split_result = torch.split(data, 3)
print("\nsplit(data, 3) 결과:", split_result) # (tensor([0,1,2]), tensor([3,4,5]), tensor([6,7,8]), tensor([9]))

# chunk(tensor, chunks): '개수'를 기준으로 나눔
# 3개의 덩어리로 최대한 균등하게 나눔 (4, 3, 3)
chunk_result = torch.chunk(data, 3)
print("chunk(data, 3) 결과:", chunk_result)
```

#### **2. 해당 설명**

**`cat`**과 **`stack`**의 차이를 명확히 이해해야 합니다.
*   **`cat` (Concatenate, 연결):** 차원의 수는 유지되고, 특정 차원의 **길이가 늘어납니다**. 흩어져 있는 데이터 조각들을 하나의 긴 데이터로 이어 붙이는 것과 같습니다.
*   **`stack` (Stack, 쌓기):** **차원의 수가 하나 늘어납니다**. 여러 개의 독립된 데이터 샘플(예: 여러 장의 이미지)들을 묶어 하나의 배치(batch)로 만드는 것과 같습니다.

**`split`**과 **`chunk`**의 차이도 중요합니다.
*   **`split`:** "각 조각의 **크기**를 얼마로 할까?"에 초점을 맞춥니다.
*   **`chunk`:** "총 몇 **개**의 조각으로 나눌까?"에 초점을 맞춥니다.
`DataLoader`에서 `batch_size`를 지정하는 것은 `split`과 유사한 개념이며, 데이터를 N개의 GPU에 분산시키는 작업은 `chunk`와 유사한 개념이라고 할 수 있습니다.

#### **3. 응용 가능한 예제**

**"다중 모달(Multi-modal) 데이터 결합"**

어떤 사용자에 대한 '나이, 성별' 같은 테이블 데이터 텐서와 '자기소개' 텍스트를 임베딩한 텐서가 있을 때, 두 텐서를 `torch.cat([...], dim=1)`을 사용해 특징(feature) 차원을 따라 연결하면, 두 종류의 정보를 모두 포함하는 하나의 확장된 특징 벡터를 만들어 모델에 입력할 수 있습니다.

#### **4. 추가하고 싶은 내용 (차원 분석)**

`cat`과 `stack`을 사용할 때, 결과 텐서의 Shape이 어떻게 될지 미리 계산해보는 습관이 중요합니다.
*   `cat([A, B], dim=d)`: A와 B의 Shape이 `d`차원을 제외하고 모두 같아야 하며, 결과 Shape의 `d`차원 크기는 `A.shape[d] + B.shape[d]`가 됩니다.
*   `stack([A, B], dim=d)`: A와 B의 Shape이 완전히 같아야 하며, 결과 Shape은 `d`차원 위치에 크기 2가 삽입된 형태가 됩니다.

#### **5. 심화 내용 (효율적인 분할)**

매우 큰 텐서를 다룰 때, `split`이나 `chunk`는 텐서의 뷰(view)를 반환하므로 메모리 복사가 일어나지 않아 효율적입니다. 이는 원본 텐서가 메모리에 그대로 있는 상태에서, 각 조각이 원본의 어느 부분을 가리키는지만 알려주는 방식이기 때문입니다.

---

### **Part 4: 고급 인덱싱 및 조건부 조작**

단순한 슬라이싱을 넘어, 특정 조건이나 인덱스 목록에 따라 텐서의 값을 선택하거나 변경하는 강력한 기술들입니다.

#### **1. 코드, 문법 및 개별 설명**

```python
# %%
# 📌 2-6 & 2-10: index_select() - 인덱스 목록으로 선택하기
x = torch.arange(12).reshape(4, 3)
print("원본 텐서 x:\n", x)
# dim=0 (행) 기준으로, 3번, 0번, 2번 행을 순서대로 선택
row_indices = torch.tensor([3, 0, 2])
selected_rows = torch.index_select(x, dim=0, index=row_indices)
print("\n선택된 행들:\n", selected_rows)


# 📌 2-11: argmax() - 최대값의 인덱스 찾기
# dim=1 (열) 기준으로, 각 행에서 최대값을 가진 원소의 인덱스를 찾음
# [ 0,  1,  2] -> max is 2, index is 2
# [ 3,  4,  5] -> max is 5, index is 2
# ...
max_indices_col = torch.argmax(x, dim=1)
print("\n각 행의 최대값 인덱스:", max_indices_col) # 결과: tensor([2, 2, 2, 2])


# 📌 2-12: topk() - 상위 K개의 값과 인덱스 찾기
y = torch.tensor([[1, 7, 3], [4, 2, 9], [8, 6, 5]])
# 각 행(dim=1)에서 가장 큰 2개(k=2)의 값과 인덱스를 찾음
values, indices = torch.topk(y, k=2, dim=1)
print("\nTop-2 값:\n", values)
print("Top-2 인덱스:\n", indices)


# 📌 2-13: masked_fill() - 조건에 따라 값 변경하기
# y 텐서에서 5보다 큰(y > 5) 원소들의 위치를 찾아, 그 위치의 값을 -1로 채움
mask = y > 5
masked_y = y.masked_fill(mask, value=-1)
print("\nmasked_fill 결과:\n", masked_y)
```

#### **2. 해당 설명**

이 함수들은 데이터를 필터링하거나, 모델의 예측 결과를 해석하고, 특정 값을 제어하는 등 매우 다양한 상황에서 활용됩니다.
*   **`index_select`:** 데이터셋에서 특정 샘플들만 추출하거나, 단어 임베딩 룩업 테이블에서 특정 단어들의 벡터만 가져올 때 사용됩니다.
*   **`argmax`:** 분류 모델의 최종 출력(각 클래스에 대한 확률 또는 점수)에서 가장 높은 값을 가진 클래스를 최종 예측 결과로 선택할 때 필수적으로 사용됩니다.
*   **`topk`:** 단순한 최대값뿐만 아니라, 상위 K개의 후보를 고려해야 할 때 유용합니다. 예를 들어, 추천 시스템에서 사용자에게 가장 관련성 높은 K개의 아이템을 추천하거나, 기계 번역에서 여러 가능한 번역 후보를 생성할 때(빔 서치, Beam Search) 사용됩니다.
*   **`masked_fill`:** 특정 조건을 만족하는 값을 다른 값으로 바꿀 때 사용합니다. 예를 들어, 어텐션 메커니즘에서 패딩(padding)된 부분에 어텐션 점수를 주지 않기 위해 해당 위치의 점수를 아주 작은 값(-inf)으로 만들어 버리는 데 핵심적으로 사용됩니다.

#### **3. 응용 가능한 예제**

**"Top-5 정확도 계산"**

이미지 분류에서 Top-1 정확도(가장 높은 확률을 가진 예측이 정답과 일치하는 비율)뿐만 아니라, Top-5 정확도(상위 5개의 예측 후보 안에 정답이 포함되는 비율)도 중요한 평가 지표입니다. `torch.topk(predictions, k=5)`를 사용하여 상위 5개 예측을 얻고, 그 안에 실제 정답이 포함되어 있는지 확인하여 Top-5 정확도를 계산할 수 있습니다.

#### **4. 추가하고 싶은 내용 (`expand`의 활용)**

제공해주신 2-9 예제의 `expand`는 브로드캐스팅을 이해하는 데 매우 좋습니다. 이는 메모리를 복사하지 않고 텐서를 확장하는 **'가상 복제'** 와 같습니다. `(2, 1, 2)` 텐서를 `expand(2, 3, 2)`로 확장하면, 메모리에는 여전히 원본 데이터만 존재하지만, PyTorch는 인덱싱 시 내부적으로 데이터를 반복하여 `(2, 3, 2)` 크기인 것처럼 동작합니다. 이는 텐서에 편향(bias)을 더하는 등의 연산에서 메모리를 극도로 효율적으로 사용하게 해주는 핵심 원리입니다.

#### **5. 심화 내용 (Gather vs. Index_select)**

`index_select`는 행이나 열 '전체'를 가져오는 반면, `torch.gather`는 각 행(또는 열)마다 내가 원하는 위치의 **'단일 원소'** 를 하나씩 가져올 수 있습니다. 훨씬 더 복잡하고 정교한 인덱싱이 필요할 때 사용됩니다. 예를 들어, `topk`로 찾은 상위 K개의 인덱스를 이용해 다른 텐서에서 해당 위치의 값들을 가져오고 싶을 때 `gather`가 유용하게 사용될 수 있습니다.