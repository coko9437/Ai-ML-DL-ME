{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-30T02:10:06.164263Z",
     "start_time": "2025-09-30T02:10:03.038450Z"
    }
   },
   "source": [
    "import torch\n",
    "print(torch.__version__)  # 설치된 PyTorch 버전 출력\n",
    "print(torch.cuda.is_available())  # CUDA 사용 가능 여부 확인\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 사용 가능: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU를 사용할 수 없습니다. CPU를 사용합니다.\")\n",
    "\n",
    "x = torch.rand(3, 3)\n",
    "y = torch.rand(3, 3)\n",
    "z = x + y\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"z:\", z)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cpu\n",
      "False\n",
      "GPU를 사용할 수 없습니다. CPU를 사용합니다.\n",
      "x: tensor([[0.4195, 0.1144, 0.3347],\n",
      "        [0.6223, 0.1585, 0.5821],\n",
      "        [0.6864, 0.6823, 0.9306]])\n",
      "y: tensor([[0.0414, 0.8769, 0.5768],\n",
      "        [0.1249, 0.3538, 0.2261],\n",
      "        [0.7561, 0.7945, 0.4646]])\n",
      "z: tensor([[0.4609, 0.9913, 0.9115],\n",
      "        [0.7472, 0.5123, 0.8082],\n",
      "        [1.4425, 1.4768, 1.3952]])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T05:33:52.989414Z",
     "start_time": "2025-09-30T05:33:52.982379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# 1. torch.randn()\n",
    "\n",
    "# 표준 정규 분포(Standard Normal Distribution)에서 랜덤 샘플 생성\n",
    "# 평균(𝜇) = 0, 표준편차(𝜎) = 1 인 정규 분포(가우시안 분포)에서 샘플링\n",
    "# 입력으로 텐서의 크기(shape)를 지정하면 해당 크기의 랜덤 텐서 생성\n",
    "# 3×3 정규 분포 텐서 생성 (평균 0, 표준편차 1)\n",
    "tensor = torch.randn(3, 3)\n",
    "print(tensor)"
   ],
   "id": "5e1499d9dc48bf86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9998,  1.0961, -0.6748],\n",
      "        [-0.2393,  1.0817, -0.0638],\n",
      "        [-0.3947,  0.2885, -0.6095]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T05:34:56.695552Z",
     "start_time": "2025-09-30T05:34:56.688434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# 2. torch.rand()\n",
    "\n",
    "# 균등 분포(Uniform Distribution)에서 랜덤 샘플 생성\n",
    "# 범위: [0, 1] 사이의 균등한 분포에서 샘플링\n",
    "\n",
    "tensor = torch.rand(3, 3)  # 0~1 사이 랜덤 값\n",
    "\n",
    "print(tensor)"
   ],
   "id": "8aeb9dc0f31f43b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7609, 0.7868, 0.6688],\n",
      "        [0.2873, 0.7709, 0.3343],\n",
      "        [0.1955, 0.1235, 0.9751]])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T05:35:19.139029Z",
     "start_time": "2025-09-30T05:35:19.124301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. torch.randint()\n",
    "#\n",
    "# 지정한 정수 범위에서 랜덤 정수 샘플 생성\n",
    "# low (최소값)과 high (최대값)을 지정하여 정수 값을 무작위 생성\n",
    "\n",
    "tensor = torch.randint(0, 10, (3, 3))  # 0~9 사이 정수 생성\n",
    "\n",
    "print(tensor)"
   ],
   "id": "84a834d2899348cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 7, 1],\n",
      "        [2, 9, 9],\n",
      "        [2, 5, 0]])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T05:35:34.378441Z",
     "start_time": "2025-09-30T05:35:34.366862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. torch.randint_like()\n",
    "# ✅ 기존 텐서와 같은 shape의 랜덤 정수 텐서 생성\n",
    "\n",
    "x = torch.ones(3, 3)  # 기존 텐서\n",
    "\n",
    "rand_tensor = torch.randint_like(x, 0, 10)  # 기존 크기 유지, 0~9 정수 샘플링\n",
    "\n",
    "print(rand_tensor)"
   ],
   "id": "1ae61c9e56fa68bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 5., 6.],\n",
      "        [3., 3., 9.],\n",
      "        [3., 3., 6.]])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T05:35:57.890707Z",
     "start_time": "2025-09-30T05:35:57.874820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. torch.normal()\n",
    "# ✅ 사용자가 지정한 평균(𝜇)과 표준편차(𝜎)로 정규 분포 샘플 생성\n",
    "\n",
    "mean = torch.tensor([0.0, 2.0, 4.0])  # 평균\n",
    "std = torch.tensor([1.0, 0.5, 0.1])  # 표준편차\n",
    "\n",
    "tensor = torch.normal(mean, std)  # 각 요소별로 지정한 평균과 표준편차 적용\n",
    "\n",
    "print(tensor)"
   ],
   "id": "401477fa222254bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1203,  1.3325,  4.1356])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T05:36:13.863415Z",
     "start_time": "2025-09-30T05:36:13.849680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. torch.randperm()\n",
    "# 지정된 범위 내에서 정수를 무작위로 섞음 (순열 생성)\n",
    "\n",
    "tensor = torch.randperm(10)  # 0~9 정수를 무작위 섞기\n",
    "print(tensor)"
   ],
   "id": "3f5388bc0ee0299b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 4, 2, 0, 5, 8, 1, 7, 6, 3])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T05:37:39.258995Z",
     "start_time": "2025-09-30T05:37:39.250563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7. torch.bernoulli()\n",
    "# 베르누이 분포(Bernoulli Distribution)에서 샘플링 (0 또는 1)\n",
    "# 입력 텐서의 각 요소를 확률로 사용하여 0 또는 1을 반환\n",
    "\n",
    "probs = torch.tensor([0.2, 0.8, 0.5, 0.9])  # 0~1 사이 확률값\n",
    "\n",
    "tensor = torch.bernoulli(probs)  # 각 확률에 따라 0 또는 1 생성\n",
    "\n",
    "print(tensor)"
   ],
   "id": "1f3fda644d28f01d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1.])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T05:39:05.102698Z",
     "start_time": "2025-09-30T05:39:05.096053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8. torch.multinomial()\n",
    "# 다항 분포(Multinomial Distribution)에서 샘플링\n",
    "# probs 텐서가 주어졌을 때, num_samples 개수만큼 랜덤 샘플을 추출\n",
    "\n",
    "probs = torch.tensor([0.1, 0.2, 0.3, 0.4])  # 각 요소의 선택 확률\n",
    "tensor = torch.multinomial(probs, 2, replacement=True)  # 2개 선택 (복원 추출)\n",
    "print(tensor)"
   ],
   "id": "ff6329f9dd160d26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T05:54:11.737924Z",
     "start_time": "2025-09-30T05:54:11.722368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 9. torch.seed()\n",
    "# ✅ 랜덤 시드 고정\n",
    "# 실험을 재현 가능하도록 하기 위해 사용\n",
    "\n",
    "torch.manual_seed(42)  # 시드 고정\n",
    "print(torch.rand(3, 3))  # 동일한 난수 생성"
   ],
   "id": "3bd4aa317c08e3d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009],\n",
      "        [0.2566, 0.7936, 0.9408]])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T05:58:13.912901Z",
     "start_time": "2025-09-30T05:58:13.903159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2-1-인플레이스(In-Place) 연산이란\n",
    "# 인플레이스(In-Place) 연산은 기존의 **텐서를 변경(덮어쓰기)**하는 연산을 의미합니다.\n",
    "# 즉, 새로운 텐서를 생성하지 않고, 기존 텐서의 메모리를 직접 수정하는 연산 방식입니다.\n",
    "\n",
    "import torch\n",
    "\n",
    "# 기존 텐서\n",
    "x = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "\n",
    "# 일반 연산 (새로운 텐서 반환)\n",
    "y = x + 10\n",
    "print(\"x (원본):\", x)  # 원본 텐서 변경 없음\n",
    "print(\"y (새로운 텐서):\", y)  # 새로운 텐서 생성됨\n",
    "\n",
    "# 인플레이스 연산\n",
    "x.add_(10)\n",
    "print(\"x (인플레이스 연산 후):\", x)  # 기존 텐서가 변경됨\n",
    "\n",
    "# 2. 주요 인플레이스 연산\n",
    "# 연산\t일반 연산\t인플레이스 연산\n",
    "# 덧셈\tx = x + 5\tx.add_(5)\n",
    "# 뺄셈\tx = x - 3\tx.sub_(3)\n",
    "# 곱셈\tx = x * 2\tx.mul_(2)\n",
    "# 나눗셈\tx = x / 4\tx.div_(4)\n",
    "# 지수 함수\tx = x.exp()\tx.exp_()\n",
    "# 정규화\tx = x.sqrt()\tx.sqrt_()"
   ],
   "id": "cbfa5e15880e5147",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (원본): tensor([1., 2., 3.])\n",
      "y (새로운 텐서): tensor([11., 12., 13.])\n",
      "x (인플레이스 연산 후): tensor([11., 12., 13.])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:13:26.820535Z",
     "start_time": "2025-09-30T06:13:26.650813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. 인플레이스 연산의 장점\n",
    "# 메모리 사용량 감소\n",
    "\n",
    "x = torch.randn(1000, 1000)\n",
    "\n",
    "# 일반 연산 (새로운 메모리 할당)\n",
    "y = x + 10  # 새로운 텐서가 생성됨\n",
    "\n",
    "# 인플레이스 연산 (메모리 절약)\n",
    "x.add_(10)  # 기존 텐서 x가 직접 변경됨\n",
    "\n",
    "# ➡ 인플레이스 연산을 사용하면 새로운 텐서를 생성하지 않으므로 메모리 사용량이 줄어듦\n",
    "\n",
    "# 4. 인플레이스 연산의 단점\n",
    "# 자동 미분(Autograd) 사용 시 문제 발생 가능\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "y = x.add_(1)  # 인플레이스 연산\n",
    "y.backward(torch.tensor([1.0, 1.0, 1.0]))  # 오류 발생 가능"
   ],
   "id": "50e94912e70a395a",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# ➡ 인플레이스 연산을 사용하면 새로운 텐서를 생성하지 않으므로 메모리 사용량이 줄어듦\u001B[39;00m\n\u001B[32m     13\u001B[39m \n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# 4. 인플레이스 연산의 단점\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# 자동 미분(Autograd) 사용 시 문제 발생 가능\u001B[39;00m\n\u001B[32m     17\u001B[39m x = torch.tensor([\u001B[32m1.0\u001B[39m, \u001B[32m2.0\u001B[39m, \u001B[32m3.0\u001B[39m], requires_grad=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m y = \u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 인플레이스 연산\u001B[39;00m\n\u001B[32m     20\u001B[39m y.backward(torch.tensor([\u001B[32m1.0\u001B[39m, \u001B[32m1.0\u001B[39m, \u001B[32m1.0\u001B[39m]))  \u001B[38;5;66;03m# 오류 발생 가능\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:09:48.879652Z",
     "start_time": "2025-09-30T06:09:48.864443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2-2-PyTorch view() 함수\n",
    "# torch.view() 함수는 PyTorch에서\n",
    "# **텐서의 크기를 변경(Reshape)**할 때 사용하는 함수입니다.\n",
    "# 하지만 view()는 기존 메모리를 유지하면서 크기만 변경하는 특징\n",
    "\n",
    "# 1. view()의 기본 문법\n",
    "\n",
    "# tensor.view(shape)\n",
    "# shape: 변경할 텐서의 크기 ((rows, columns, ...))\n",
    "\n",
    "# 2. view() 예제: 1D → 2D 변환\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.arange(6)  # 1D 텐서 (0~5)\n",
    "print(\"Original Shape:\", x.shape)\n",
    "\n",
    "xreshaped = x.view(2, 3)  # 2행 3열로 변경\n",
    "\n",
    "print(\"Reshaped Shape:\", xreshaped.shape)\n",
    "print(xreshaped)"
   ],
   "id": "3145e1d4979901eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape: torch.Size([6])\n",
      "Reshaped Shape: torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:10:03.124293Z",
     "start_time": "2025-09-30T06:10:03.116911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. view(-1): 자동 크기 계산\n",
    "# -1을 사용하면 PyTorch가 자동으로 크기를 계산하여 설정합니다.\n",
    "\n",
    "x = torch.arange(12)\n",
    "xreshaped = x.view(3, -1)  # 열(-1)은 자동 계산됨\n",
    "\n",
    "print(xreshaped.shape)\n",
    "print(xreshaped)"
   ],
   "id": "8616e0cb7d297ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:14:05.305728Z",
     "start_time": "2025-09-30T06:14:05.294780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. 3D → 2D 변환 예제\n",
    "x = torch.arange(24).view(2, 3, 4)  # 3D 텐서 생성 (배치=2, 행=3, 열=4)\n",
    "\n",
    "print(\"3D Tensor Shape:\", x.shape)\n",
    "\n",
    "xflattened = x.view(2, -1)  # 배치 유지, 나머지는 자동 조정\n",
    "print(\"Flattened Shape:\", xflattened.shape)\n",
    "\n",
    "# 5. view() 사용 시 주의할 점\n",
    "# 연속된 메모리(Contiguous) 필요\n",
    "\n",
    "# view()는 메모리 연속성이 유지되는 경우에만 사용 가능\n",
    "# 만약 연속적이지 않다면 .contiguous().view()를 사용해야 함\n",
    "\n",
    "\n",
    "x = torch.randn(2, 3, 4)  # (Batch=2, Rows=3, Cols=4)\n",
    "xt = x.transpose(0, 1)    # (Rows=3, Batch=2, Cols=4) → 메모리 비연속"
   ],
   "id": "ed5433ec032c0413",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Tensor Shape: torch.Size([2, 3, 4])\n",
      "Flattened Shape: torch.Size([2, 12])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 6. view()와 reshape() 차이\n",
    "# 함수\t특징\n",
    "# view()\t메모리 연속성 유지 필요, 속도 빠름\n",
    "# reshape()\t메모리 연속성 없어도 사용 가능\n",
    "\n",
    "x = torch.randn(2, 3)\n",
    "xt = x.transpose(0, 1)  # 차원 변경 → 메모리 비연속\n",
    "\n",
    "# view() 오류 발생\n",
    "try:\n",
    "    xview = xt.view(6)\n",
    "except RuntimeError as e:\n",
    "    print(\"view() Error:\", e)\n",
    "\n",
    "# reshape()는 자동으로 해결\n",
    "xreshape = xt.reshape(6)\n",
    "print(\"reshape() 성공:\", xreshape.shape)\n",
    "\n",
    "# ✅ 메모리 연속성이 없는 경우 view()는 실패하지만 reshape()는 가능!"
   ],
   "id": "fd85a639088ac8e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:16:34.657488Z",
     "start_time": "2025-09-30T06:16:34.646653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2-3-squeeze() vs unsqueeze() 함수 차이점\n",
    "# PyTorch에서 squeeze()와 unsqueeze()는 텐서의 차원을 변경하는 함수입니다.\n",
    "\n",
    "# squeeze(): 크기가 1인 차원 제거\n",
    "\n",
    "# squeeze()는 크기가 1인 차원을 자동으로 제거합니다.\n",
    "import torch\n",
    "\n",
    "x = torch.randn(1, 3, 1, 4)  # (Batch=1, Channels=3, Height=1, Width=4)\n",
    "\n",
    "x_squeezed = x.squeeze()  # 크기가 1인 차원을 제거\n",
    "\n",
    "print(x.shape)          # torch.Size([1, 3, 1, 4])\n",
    "print(x_squeezed.shape) # torch.Size([3, 4])\n",
    "\n",
    "x_squeezed_1 = x.squeeze(0)  # 0번 차원(크기가 1인 경우만) 제거\n",
    "x_squeezed_2 = x.squeeze(2)  # 2번 차원(크기가 1인 경우만) 제거\n",
    "\n",
    "print(x_squeezed_1.shape)  # torch.Size([3, 1, 4])\n",
    "print(x_squeezed_2.shape)  # torch.Size([1, 3, 4])"
   ],
   "id": "1f9c9b54ae4a08be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1, 4])\n",
      "torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:39:26.896260Z",
     "start_time": "2025-09-30T06:39:26.888914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = torch.randn(3, 4)  # (3, 4) 형태\n",
    "y_unsqueezed = y.unsqueeze(0)  # 0번 차원 추가\n",
    "\n",
    "print(y.shape)           # torch.Size([3, 4])\n",
    "print(y_unsqueezed.shape) # torch.Size([1, 3, 4])"
   ],
   "id": "d284d60b33cb5b44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:23:30.137096Z",
     "start_time": "2025-09-30T06:23:30.128295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2D 텐서 생성 (2x3 크기)\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (2,3)\n",
    "y = torch.tensor([[7, 8, 9], [10, 11, 12]])  # Shape: (2,3)\n",
    "\n",
    "# unsqueeze(0)을 통해 새로운 차원 추가 후 cat() 적용\n",
    "z = torch.cat([x.unsqueeze(0), y.unsqueeze(0)], dim=0)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Original x shape:\", x.shape)\n",
    "print(\"Original y shape:\", y.shape)\n",
    "print(\"After unsqueeze and cat, z shape:\", z.shape)\n",
    "print(\"z Tensor:\\n\", z)"
   ],
   "id": "a76f5cf2501aaefe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x shape: torch.Size([2, 3])\n",
      "Original y shape: torch.Size([2, 3])\n",
      "After unsqueeze and cat, z shape: torch.Size([2, 2, 3])\n",
      "z Tensor:\n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:38:29.387387Z",
     "start_time": "2025-09-30T06:38:29.377099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. 텐서 크기(Shape) 이해\n",
    "# PyTorch에서 tensor.shape을 확인하면 텐서의 차원과 크기를 알 수 있습니다.\n",
    "\n",
    "import torch\n",
    "\n",
    "# 0D 텐서 (스칼라)\n",
    "scalar = torch.tensor(42)\n",
    "print(\"Scalar Shape:\", scalar.shape)  # ()\n",
    "\n",
    "# 1D 텐서 (벡터)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "print(\"Vector Shape:\", vector.shape)  # (3,)\n",
    "\n",
    "# 2D 텐서 (행렬)\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"Matrix Shape:\", matrix.shape)  # (2, 3)\n",
    "\n",
    "# 3D 텐서 (다차원 배열)\n",
    "tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "print(\"3D Tensor Shape:\", tensor_3d.shape)  # (2, 2, 2)"
   ],
   "id": "9a0a2e8fdf60071f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar Shape: torch.Size([])\n",
      "Vector Shape: torch.Size([3])\n",
      "Matrix Shape: torch.Size([2, 3])\n",
      "3D Tensor Shape: torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:54:05.921889Z",
     "start_time": "2025-09-30T06:54:05.912111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. 텐서 요소 변화 예시\n",
    "\n",
    "# 1) reshape()을 활용한 형태 변경\n",
    "\n",
    "x = torch.arange(6)  # [0, 1, 2, 3, 4, 5]\n",
    "print(\"Original Shape:\", x.shape)  # (6,)\n",
    "print(\"------------\")\n",
    "x_reshaped = x.reshape(2, 3)  # 2행 3열로 변경\n",
    "print(\"Reshaped Shape:\", x_reshaped.shape)  # (2, 3)\n",
    "print(x_reshaped)\n",
    "\n",
    "# reshape을 사용하면 텐서의 shape을 변경 가능\n",
    "# 데이터의 개수(요소 개수)는 그대로 유지되지만, 차원만 변경됨"
   ],
   "id": "75dfc60dbf95e832",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape: torch.Size([6])\n",
      "------------\n",
      "Reshaped Shape: torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:54:52.230622Z",
     "start_time": "2025-09-30T06:54:52.223316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2) unsqueeze()를 활용한 차원 추가\n",
    "\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(\"Original Shape:\", x.shape)  # (3,)\n",
    "print(\"------------\")\n",
    "x_unsqueezed = x.unsqueeze(0)  # 차원 추가 (배치 차원 추가)\n",
    "print(\"Unsqueezed Shape:\", x_unsqueezed.shape)  # (1, 3)\n",
    "\n",
    "# unsqueeze(dim=0)을 사용하면 (3,) → (1, 3)로 차원이 추가됨\n",
    "# CNN 모델에서 배치 차원을 추가할 때 자주 사용됨"
   ],
   "id": "99644622d0b72739",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape: torch.Size([3])\n",
      "------------\n",
      "Unsqueezed Shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:55:52.183975Z",
     "start_time": "2025-09-30T06:55:52.172558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3) squeeze()를 활용한 차원 축소\n",
    "\n",
    "\n",
    "x = torch.tensor([[1, 2, 3]])  # (1, 3)\n",
    "print(\"Original Shape:\", x.shape)\n",
    "print(\"------------\")\n",
    "x_squeezed = x.squeeze()\n",
    "print(\"Squeezed Shape:\", x_squeezed.shape)  # (3,)\n",
    "print(\"------------\")\n",
    "# 4) permute()를 활용한 차원 순서 변경\n",
    "\n",
    "x = torch.randn(3, 4, 5)  # (채널, 높이, 너비)\n",
    "print(\"Original Shape:\", x.shape)  # (3, 4, 5)\n",
    "print(\"------------\")\n",
    "x_permuted = x.permute(1, 2, 0)  # (높이, 너비, 채널)로 변경\n",
    "\n",
    "print(\"Permuted Shape:\", x_permuted.shape)  # (4, 5, 3)"
   ],
   "id": "e118384ad1e3d54d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape: torch.Size([1, 3])\n",
      "------------\n",
      "Squeezed Shape: torch.Size([3])\n",
      "------------\n",
      "Original Shape: torch.Size([3, 4, 5])\n",
      "------------\n",
      "Permuted Shape: torch.Size([4, 5, 3])\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:56:05.379261Z",
     "start_time": "2025-09-30T06:56:05.370019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. 텐서 연산을 통한 요소 변화\n",
    "\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "print(\"Original Tensor:\")\n",
    "print(x)\n",
    "print(\"------------\")\n",
    "# 텐서 값 변경 (연산 수행)\n",
    "y = x * 2  # 요소별 연산 (모든 원소를 2배)\n",
    "print(\"After Multiplication by 2:\")\n",
    "print(y)\n",
    "print(\"------------\")\n",
    "# 요소별 제곱 연산\n",
    "z = x ** 2\n",
    "print(\"After Squaring:\")\n",
    "print(z)\n",
    "\n",
    "# 0D 텐서 (스칼라):  숫자 하나\n",
    "# 1D 텐서 (벡터):   리스트 형태의 배열\n",
    "# 2D 텐서 (행렬):   행과 열이 있는 배열\n",
    "# 3D 이상 텐서:     다차원 배열\n",
    "\n",
    "# reshape()\t    크기 변경 (요소 개수 유지)\t(6,) → (2, 3)\n",
    "# unsqueeze()\t차원 추가\t(3,) → (1, 3)\n",
    "# squeeze()\t    차원 제거\t(1, 3, 1) → (3,)\n",
    "# permute()\t    차원 순서 변경\t(3, 4, 5) → (4, 5, 3)"
   ],
   "id": "226a174cffdefd92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "------------\n",
      "After Multiplication by 2:\n",
      "tensor([[2, 4],\n",
      "        [6, 8]])\n",
      "------------\n",
      "After Squaring:\n",
      "tensor([[ 1,  4],\n",
      "        [ 9, 16]])\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T06:59:39.910175Z",
     "start_time": "2025-09-30T06:59:39.893461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 입력 데이터 X와 정답 데이터 Y의 shape\n",
    "\n",
    "X = torch.tensor([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]], dtype=torch.float32)  # 입력값\n",
    "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)  # 정답값\n",
    "\n",
    "# 데이터\t내용\tShape\n",
    "# X\t4개의 샘플, 각 샘플은 2개의 입력 특성(0 또는 1)\t(4, 2)\n",
    "# Y\t4개의 샘플, 각 샘플의 정답(0 또는 1)\t(4, 1)\n",
    "\n",
    "# 즉, X는 (batch_size, input_dim) = (4, 2) 형태의 입력을 가짐\n",
    "# 정답 Y는 (batch_size, output_dim) = (4, 1) 형태의 출력을 기대함\n",
    "\n",
    "\n",
    "# 2. forward() 함수에서 데이터 흐름과 shape 변화\n",
    "\n",
    "# def forward(self, x):\n",
    "#     x = self.activation(self.hidden(x))  # 은닉층 활성화\n",
    "#     x = self.activation(self.output(x))  # 출력층 활성화\n",
    "#     return x\n",
    "\n",
    "# (1) 입력층 → 은닉층 변환 (self.hidden(x))\n",
    "\n",
    "# self.hidden = nn.Linear(2, 2)  # (input_dim=2 → hidden_dim=2)\n",
    "# X.shape = (4, 2) → hidden(x)을 통과하면 (4, 2)\n",
    "# 선형 변환 공식: Z1 = X * W1 + b1\n",
    "\n",
    "# (2) 은닉층 활성화 (self.activation(hidden(x)))\n",
    "\n",
    "# 활성화 함수 Sigmoid 적용 → shape 변화 없음\n",
    "# x.shape = (4, 2)\n",
    "\n",
    "# (3) 은닉층 → 출력층 변환 (self.output(x))\n",
    "\n",
    "# self.output = nn.Linear(2, 1)  # (hidden_dim=2 → output_dim=1)\n",
    "# x.shape = (4, 2) → output(x)을 통과하면 (4, 1)\n",
    "\n",
    "# 선형 변환 공식: Z2 = A1 * W2 + b2\n",
    "# => (4,2) × (2,1) = (4,1)\n",
    "\n",
    "# (4) 출력층 활성화 (self.activation(output(x)))\n",
    "# 활성화 함수 Sigmoid 적용 → shape 변화 없음\n",
    "# 최종 출력 x.shape = (4, 1)\n",
    "\n",
    "# 3. 데이터 흐름 Shape 변화 요약\n",
    "\n",
    "# 레이어\t연산\tShape 변화\n",
    "\n",
    "# 입력 데이터\tX\t                                :(4, 2)\n",
    "# 은닉층 선형 변환\tself.hidden(x)\t                :(4, 2)\n",
    "# 은닉층 활성화 함수\tself.activation(hidden(x))\t:(4, 2)\n",
    "# 출력층 선형 변환\tself.output(x)\t                :(4, 1)\n",
    "# 출력층 활성화 함수\tself.activation(output(x))\t:(4, 1)\n",
    "# 최종 출력\tY_hat\t                            :(4, 1)\n",
    "\n",
    "# 4. 실제 shape 확인 코드\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class XOR_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XOR_NN, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 2)  # 입력 2 → 은닉층 2\n",
    "        self.output = nn.Linear(2, 1)  # 은닉층 2 → 출력층 1\n",
    "        self.activation = nn.Sigmoid()  # 활성화 함수\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"입력 데이터 shape:\", x.shape)  # (4, 2)\n",
    "        x = self.hidden(x)\n",
    "        print(\"은닉층 통과 후 shape:\", x.shape)  # (4, 2)\n",
    "        x = self.activation(x)\n",
    "        print(\"은닉층 활성화 후 shape:\", x.shape)  # (4, 2)\n",
    "        x = self.output(x)\n",
    "        print(\"출력층 통과 후 shape:\", x.shape)  # (4, 1)\n",
    "        x = self.activation(x)\n",
    "        print(\"출력층 활성화 후 shape:\", x.shape)  # (4, 1)\n",
    "        return x\n",
    "\n",
    "# 모델 생성\n",
    "model = XOR_NN()\n",
    "\n",
    "# 입력 데이터\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "\n",
    "# 모델 실행\n",
    "output = model(X)"
   ],
   "id": "bee218a89d6cf3dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 shape: torch.Size([4, 2])\n",
      "은닉층 통과 후 shape: torch.Size([4, 2])\n",
      "은닉층 활성화 후 shape: torch.Size([4, 2])\n",
      "출력층 통과 후 shape: torch.Size([4, 1])\n",
      "출력층 활성화 후 shape: torch.Size([4, 1])\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T07:00:27.801509Z",
     "start_time": "2025-09-30T07:00:27.793721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3차원 텐서의 기본 개념\n",
    "# 3차원 텐서는 다음과 같은 **3개의 차원(Dimension)**을 가집니다.\n",
    "\n",
    "# 3D Tensor Shape: (D1, D2, D3)\n",
    "# 첫 번째 차원 (D_1) → 일반적으로 배치 크기(Batch Size), 채널 수(Channel)\n",
    "# 두 번째 차원 (D_2) → 행(Row) 또는 높이(Height)\n",
    "# 세 번째 차원 (D_3) → 열(Column) 또는 너비(Width)\n",
    "\n",
    "# 1. 3D 텐서 생성 및 각 인덱스 별 위치 확인\n",
    "\n",
    "import torch\n",
    "\n",
    "# 3D 텐서 생성 (배치=2, 높이=3, 너비=4)\n",
    "tensor_3d = torch.tensor([\n",
    "    [[1, 2, 3, 4],\n",
    "     [5, 6, 7, 8],\n",
    "     [9, 10, 11, 12]],  # 첫 번째 배치\n",
    "\n",
    "    [[13, 14, 15, 16],\n",
    "     [17, 18, 19, 20],\n",
    "     [21, 22, 23, 24]]  # 두 번째 배치\n",
    "])\n",
    "\n",
    "print(\"3D Tensor Shape:\", tensor_3d.shape)  # (2, 3, 4)"
   ],
   "id": "f0e9bffedf6613bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Tensor Shape: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T07:01:04.816670Z",
     "start_time": "2025-09-30T07:01:04.806030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. 각 차원별 인덱스 위치\n",
    "\n",
    "# 텐서에서 특정 인덱스의 값을 참조할 때 (배치, 높이, 너비) 순서로 접근합니다.\n",
    "# tensor_3d = torch.tensor([\n",
    "#     [[1, 2, 3, 4],\n",
    "#      [5, 6, 7, 8],\n",
    "#      [9, 10, 11, 12]],  # 첫 번째 배치\n",
    "#-----------------------------------\n",
    "#     [[13, 14, 15, 16],\n",
    "#      [17, 18, 19, 20],\n",
    "#      [21, 22, 23, 24]]  # 두 번째 배치\n",
    "# ])\n",
    "print(\"tensor_3d[0, 0, 0]:\", tensor_3d[0, 0, 0])  #첫 번째 배치, 첫 번째 행, 첫 번째 열\t1\n",
    "print(\"tensor_3d[0, 0, 1]:\", tensor_3d[0, 0, 1])  #첫 번째 배치, 첫 번째 행, 두 번째 열\t2\n",
    "print(\"tensor_3d[0, 1, 2]:\", tensor_3d[0, 1, 2])  #첫 번째 배치, 두 번째 행, 세 번째 열\t7\n",
    "print(\"tensor_3d[1, 2, 3]:\", tensor_3d[1, 2, 3])  #두 번째 배치, 세 번째 행, 네 번째 열\t24"
   ],
   "id": "83f066ad8da5ecc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_3d[0, 0, 0]: tensor(1)\n",
      "tensor_3d[0, 0, 1]: tensor(2)\n",
      "tensor_3d[0, 1, 2]: tensor(7)\n",
      "tensor_3d[1, 2, 3]: tensor(24)\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T07:14:14.330785Z",
     "start_time": "2025-09-30T07:14:14.322055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# (1) 특정 배치 선택 (tensor_3d[0])\n",
    "\n",
    "print(tensor_3d[0])  # 첫 번째 배치 데이터 출력\n",
    "# tensor([[ 1,  2,  3,  4],\n",
    "#         [ 5,  6,  7,  8],\n",
    "#         [ 9, 10, 11, 12]])\n",
    "\n",
    "print(\"--------------------------\")\n",
    "# (2) 특정 행 선택 (tensor_3d[1, 1])\n",
    "\n",
    "print(tensor_3d[1, 1])\n",
    "print(\"--------------------------\")\n",
    "# (3) 특정 열 선택 (tensor_3d[:, :, 2])\n",
    "\n",
    "print(tensor_3d[:, :, 2]) # 모든 배치에서 3번째 열 선택"
   ],
   "id": "14e6d10e90fef893",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "--------------------------\n",
      "tensor([17, 18, 19, 20])\n",
      "--------------------------\n",
      "tensor([[ 3,  7, 11],\n",
      "        [15, 19, 23]])\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T07:14:31.744145Z",
     "start_time": "2025-09-30T07:14:31.737373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. permute()를 활용한 위치 변경\n",
    " # permute(1, 2, 0): (배치, 높이, 너비) → (높이, 너비, 배치)\n",
    "\n",
    "\n",
    "tensor_permuted = tensor_3d.permute(1, 2, 0)  # (높이, 너비, 배치)\n",
    "print(\"Permuted Shape:\", tensor_permuted.shape)\n",
    "# ➡ 배치가 마지막으로 이동하고, 높이가 첫 번째 차원으로 변경됨\n",
    "\n",
    "# 다른예시: permute(2, 0, 1): (배치, 높이, 너비) → (너비, 배치, 높이)\n",
    "\n",
    "# 5. 결론\n",
    " # 3D 텐서는 (배치, 높이, 너비) = (D_1, D_2, D_3) 형태\n",
    " # 인덱싱(tensor_3d[i, j, k])을 사용하여 특정 위치 선택 가능\n",
    " # permute()를 사용하면 차원의 순서를 변경 가능\n",
    " # 특정 차원 선택 시 :를 활용하여 전체 배치 또는 특정 행/열만 가져올 수 있음"
   ],
   "id": "b83e74b0509c4437",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permuted Shape: torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2-5-split() vs chunk() 차이점\n",
    "\n",
    " # split() vs chunk() 차이점\n",
    "# torch.split()과 torch.chunk()는\n",
    " # PyTorch에서 텐서를 특정 크기 또는 개수로 나누는 함수"
   ],
   "id": "ebac98a08cd8d03d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T07:17:03.907255Z",
     "start_time": "2025-09-30T07:17:03.888052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split() 함수\n",
    "# split(batch_size, dim) : 주어진 크기(batch_size)로 텐서를 나눔.\n",
    "# 마지막 부분의 크기는 남은 요소 개수에 따라 조정될 수 있음.\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.FloatTensor(10, 4)  # (10, 4) 크기의 텐서 생성\n",
    "\n",
    "splits = x.split(4, dim=0)  # 첫 번째 차원(행 기준)으로 크기 4씩 나눔\n",
    "\n",
    "for s in splits:\n",
    "    print(s.size())\n",
    "    # 마지막 조각은 크기가 다를 수 있음. (총 10개 데이터를 4개씩 나누면 4+4+2)"
   ],
   "id": "b62895886faf527b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T07:18:04.650136Z",
     "start_time": "2025-09-30T07:18:04.639854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# chunk() 함수\n",
    " # chunk(n_chunks, dim): 지정한 개수(n_chunks)로 텐서를 균등하게 나눔.\n",
    " #    split()과 다르게 크기가 자동으로 균등하게 배분됨.\n",
    "\n",
    "x = torch.FloatTensor(8, 4)  # (8, 4) 크기의 텐서 생성\n",
    "chunks = x.chunk(3, dim=0)  # 3개로 나눔\n",
    "\n",
    "for c in chunks:\n",
    "    print(c.size())\n",
    "    # split()과 유사하지만, 균등하게 분할할 수 있도록 자동으로 크기를 맞춤.\n",
    "\n",
    "# -> 정확한 크기(batch_size)로 나누려면 split() 사용.\n",
    "    # 균등하게 n개로 나누려면 chunk() 사용."
   ],
   "id": "43637ae1ac23c80e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T07:23:14.536514Z",
     "start_time": "2025-09-30T07:23:14.526737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# index_select()\n",
    "    # 특정 차원의 특정 인덱스만 선택하여 새로운 텐서를 생성하는 함수\n",
    "# torch.index_select(input, dim, index)\n",
    "#                     input : 원본 텐서\n",
    "#                         dim : 선택할 차원 (0=행, 1=열, ...)\n",
    "#                             index : 선택할 인덱스 리스트\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.arange(12).reshape(4, 3)  # (4,3) 텐서 생성\n",
    "print(\"Original Tensor:\\n\", x)\n",
    "print(\"---------------------------\")\n",
    "\n",
    "index = torch.tensor([1, 3], dtype=torch.long) # 2번째, 4번째 행 선택\n",
    "\n",
    "# 행 선택 (dim=0 -> 행 방향)\n",
    "selected_rows = torch.index_select(x, dim=0, index=index)\n",
    "\n",
    "print(\"Selected Rows:\\n\", selected_rows)\n",
    "print(\"---------------------------\")\n",
    "# 열 선택 (dim=1 → 열 방향으로 선택)\n",
    "index = torch.tensor([0, 2], dtype=torch.long)  # 1 번째, 3 번째 열 선택\n",
    "selected_cols = torch.index_select(x, dim=1, index=index)\n",
    "\n",
    "print(\"Selected Columns:\\n\", selected_cols)"
   ],
   "id": "1a055d00890ded8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      " tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "---------------------------\n",
      "Selected Rows:\n",
      " tensor([[ 3,  4,  5],\n",
      "        [ 9, 10, 11]])\n",
      "---------------------------\n",
      "Selected Columns:\n",
      " tensor([[ 0,  2],\n",
      "        [ 3,  5],\n",
      "        [ 6,  8],\n",
      "        [ 9, 11]])\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# index_select() vs tensor[indices] 차이\n",
    "\n",
    "# 직접 인덱싱\n",
    "selected_rows = x[[1, 3]]\n",
    "print(selected_rows)\n",
    "\n",
    "# index_select()는 메모리 연속성이 유지됨.\n",
    "# x[[...]]는 메모리 비연속적인 텐서를 생성할 수 있음.\n",
    "\n",
    "# index_select(x, dim=0, index) / dim=0 행\n",
    "\n",
    "# index_select(x, dim=1, index) / dim=1 열\n",
    "\n",
    "# x[[indices]]\t특정 행 선택 (직접 인덱싱)\tx[[1,3]]"
   ],
   "id": "a5c5d3694e5914e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T08:05:32.203834Z",
     "start_time": "2025-09-30T08:05:32.193265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.cat() 함수\n",
    "    # 여러 개의 텐서를 특정 차원(dim)으로 연결(Concatenate)하는 함수\n",
    "# torch.cat(tensors, dim)\n",
    "            # tensors : 연결할 텐서들의 리스트 [tensor1, tensor2, ...]\n",
    "                    # dim : 연결할 차원 (0=행 방향, 1=열 방향, ...)\n",
    "\n",
    "# 행 방향(dim=0)으로 연결\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]])  # (2,2)\n",
    "b = torch.tensor([[5, 6], [7, 8]])  # (2,2)\n",
    "\n",
    "# 행 방향(dim=0)으로 연결\n",
    "c = torch.cat([a, b], dim=0)\n",
    "\n",
    "print(\"Concatenated Tensor (dim=0):\\n\", c)\n",
    "print(\"---------------------------\")\n",
    "print(\"Shape:\", c.shape)\n",
    "# (2,2) + (2,2) → (4,2)... 행이 늘어남(수직 방향으로 연결).\n",
    "print(\"---------------------------\")\n",
    "# 열 방향(dim=1)으로 연결\n",
    "c = torch.cat([a, b], dim=1)  # 열 방향으로 연결\n",
    "\n",
    "print(\"Concatenated Tensor (dim=1):\\n\", c)\n",
    "print(\"---------------------------\")\n",
    "print(\"Shape:\", c.shape)\n",
    "\n",
    "# cat()을 사용할 때 주의할 점\n",
    "    # 차원이 동일해야 함. dim 방향만 다를 수 있음.\n",
    "\n",
    "    # 오류 예제\n",
    "\n",
    "    # a = torch.tensor([[1, 2]])  # (1,2)\n",
    "    # b = torch.tensor([[3], [4]])  # (2,1)\n",
    "    # -> c = torch.cat([a, b], dim=0)  # 서로 다른 shape → 오류 발생!"
   ],
   "id": "55808a2a5d28249d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Tensor (dim=0):\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "---------------------------\n",
      "Shape: torch.Size([4, 2])\n",
      "---------------------------\n",
      "Concatenated Tensor (dim=1):\n",
      " tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n",
      "Shape: torch.Size([2, 4])\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2-8-torch.stack() : 새로운 차원을 추가하면서 여러 텐서를 쌓는(stack) 함수\n",
    "# torch.stack(tensors, dim)\n",
    "            # tensors : 스택할 텐서들의 리스트 [tensor1, tensor2, ...]\n",
    "                    # dim : 새로운 차원을 추가할 위치\n",
    "\n",
    "# cat()은 기존 차원에서 연결 (새로운 차원 추가 ❌).\n",
    "# stack()은 새로운 차원 추가하면서 연결 (새로운 차원 추가 ⭕)."
   ],
   "id": "cf1d9d7c0ab0ea0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T08:26:10.800259Z",
     "start_time": "2025-09-30T08:26:10.791021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])  # (3,)\n",
    "b = torch.tensor([4, 5, 6])  # (3,)\n",
    "\n",
    "c = torch.stack([a, b], dim=0)  # 새로운 차원 추가\n",
    "\n",
    "print(\"Stacked Tensor (dim=0):\\n\", c)\n",
    "\n",
    "print(\"Shape:\", c.shape)\n",
    "# 결과 shape: (2,3) → 기존 (3,) 텐서 2개를 쌓아서 (2,3) 텐서 생성.\n",
    "print(\"---------------------------\")\n",
    "c = torch.stack([a, b], dim=1)  # 새로운 차원을 1번 위치에 추가\n",
    "\n",
    "print(\"Stacked Tensor (dim=1):\\n\", c)\n",
    "\n",
    "print(\"Shape:\", c.shape)"
   ],
   "id": "cd38dbb4a6db516e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Tensor (dim=0):\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Shape: torch.Size([2, 3])\n",
      "---------------------------\n",
      "Stacked Tensor (dim=1):\n",
      " tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "Shape: torch.Size([3, 2])\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T08:26:28.802609Z",
     "start_time": "2025-09-30T08:26:28.794383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2D 텐서에 적용\n",
    "a = torch.tensor([[1, 2], [3, 4]])  # (2,2)\n",
    "b = torch.tensor([[5, 6], [7, 8]])  # (2,2)\n",
    "\n",
    "c = torch.stack([a, b], dim=0)\n",
    "print(\"Stacked Tensor (dim=0):\\n\", c)\n",
    "print(\"Shape:\", c.shape)\n",
    "# dim=0으로 stack하면 (2,2) → (2,2,2) (새로운 차원 추가됨)"
   ],
   "id": "81c2a540036f07fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Tensor (dim=0):\n",
      " tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "Shape: torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T08:30:54.767670Z",
     "start_time": "2025-09-30T08:30:54.754511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2-9-expand() 예제\n",
    "\n",
    "import torch\n",
    "\n",
    "# 3D 텐서 생성 (2, 1, 2)\n",
    "x = torch.FloatTensor([[[1, 2]], [[3, 4]]])\n",
    "print(\"Original x shape:\", x.shape)\n",
    "print(\"---------------------------\")\n",
    "# expand()를 사용하여 새로운 차원 확장 (2, 3, 2)\n",
    "y = x.expand(2, 3, 2)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Expanded y shape:\", y.shape)\n",
    "print(\"---------------------------\")\n",
    "print(\"Expanded y Tensor:\\n\", y)"
   ],
   "id": "bfe523f83ea607c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x shape: torch.Size([2, 1, 2])\n",
      "Expanded y shape: torch.Size([2, 3, 2])\n",
      "Expanded y Tensor:\n",
      " tensor([[[1., 2.],\n",
      "         [1., 2.],\n",
      "         [1., 2.]],\n",
      "\n",
      "        [[3., 4.],\n",
      "         [3., 4.],\n",
      "         [3., 4.]]])\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T08:36:45.448008Z",
     "start_time": "2025-09-30T08:36:45.430111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2-10-torch.randperm() :0부터 n-1까지의 숫자를 랜덤한 순서로 정렬한 텐서를 반환\n",
    "\n",
    "import torch\n",
    "\n",
    "# 0부터 9까지의 숫자를 무작위로 섞기 (순열 생성)\n",
    "perm = torch.randperm(10)\n",
    "\n",
    "print(\"Random Permutation:\", perm)\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# 데이터셋의 인덱스 랜덤 섞기\n",
    "data = torch.tensor([10, 20, 30, 40, 50])\n",
    "indices = torch.randperm(len(data))\n",
    "\n",
    "shuffled_data = data[indices]\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Shuffled Data:\", shuffled_data)"
   ],
   "id": "ee4b26531cee2bf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Permutation: tensor([7, 3, 4, 9, 6, 1, 0, 5, 2, 8])\n",
      "---------------------------\n",
      "Original Data: tensor([10, 20, 30, 40, 50])\n",
      "Shuffled Data: tensor([20, 30, 10, 40, 50])\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T08:37:49.307390Z",
     "start_time": "2025-09-30T08:37:49.296907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2-11-torch.argmax(dim=n)\n",
    "\n",
    "import torch\n",
    "\n",
    "# 3x3 텐서 생성\n",
    "x = torch.tensor([[1, 7, 3],\n",
    "                  [4, 2, 9],\n",
    "                  [8, 6, 5]])\n",
    "\n",
    "# argmax(dim=0): 열 방향(세로)에서 최대값의 인덱스 찾기\n",
    "argmax_dim0 = torch.argmax(x, dim=0)\n",
    "\n",
    "# argmax(dim=1): 행 방향(가로)에서 최대값의 인덱스 찾기\n",
    "argmax_dim1 = torch.argmax(x, dim=1)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Tensor:\\n\", x)\n",
    "print(\"\\nArgmax along dim=0 (column-wise):\", argmax_dim0)\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Argmax along dim=1 (row-wise):\", argmax_dim1)\n",
    "\n",
    "\n",
    "\n",
    "# torch.argmax(x, dim=0) → 열 방향 (dim=0)에서 최대값의 인덱스 반환\n",
    "\n",
    "# 첫 번째 열 [1, 4, 8] → 최대값: 8 (인덱스 2)\n",
    "# 두 번째 열 [7, 2, 6] → 최대값: 7 (인덱스 0)\n",
    "# 세 번째 열 [3, 9, 5] → 최대값: 9 (인덱스 1)\n",
    "# 결과: tensor([2, 0, 1])\n",
    "\n",
    "# torch.argmax(x, dim=1) → 행 방향 (dim=1)에서 최대값의 인덱스 반환\n",
    "\n",
    "# 첫 번째 행 [1, 7, 3] → 최대값: 7 (인덱스 1)\n",
    "# 두 번째 행 [4, 2, 9] → 최대값: 9 (인덱스 2)\n",
    "# 세 번째 행 [8, 6, 5] → 최대값: 8 (인덱스 0)\n",
    "# 결과: tensor([1, 2, 0])"
   ],
   "id": "9bb8f2233809f329",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      " tensor([[1, 7, 3],\n",
      "        [4, 2, 9],\n",
      "        [8, 6, 5]])\n",
      "\n",
      "Argmax along dim=0 (column-wise): tensor([2, 0, 1])\n",
      "---------------------------------------\n",
      "Argmax along dim=1 (row-wise): tensor([1, 2, 0])\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T08:38:55.674424Z",
     "start_time": "2025-09-30T08:38:55.665589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2-12-torch.topk()\n",
    "\n",
    "import torch\n",
    "\n",
    "# 3x3 텐서 생성\n",
    "x = torch.tensor([[1, 7, 3],\n",
    "                  [4, 2, 9],\n",
    "                  [8, 6, 5]])\n",
    "\n",
    "# 각 행(row)에서 최대값 1개(`k=1`) 찾기 (dim=-1 → 마지막 차원 기준)\n",
    "    # dim=-1은 마지막 차원(여기서는 행 방향)을 기준으로 동작.\n",
    "topk_values, topk_indices = torch.topk(x, k=1, dim=-1)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Tensor:\\n\", x)\n",
    "print(\"\\nTop-1 values along dim=-1 (row-wise):\\n\", topk_values)\n",
    "print(\"Top-1 indices along dim=-1:\", topk_indices)\n",
    "\n",
    "# topk_values: 각 행에서 최대값 반환.\n",
    "# topk_indices: 각 행에서 최대값의 인덱스 반환.\n",
    "\n",
    "# torch.topk()를 사용하면 k개의 최댓값과 해당 인덱스를 쉽게 찾을 수 있음."
   ],
   "id": "bb518390e2ab3a06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      " tensor([[1, 7, 3],\n",
      "        [4, 2, 9],\n",
      "        [8, 6, 5]])\n",
      "\n",
      "Top-1 values along dim=-1 (row-wise):\n",
      " tensor([[7],\n",
      "        [9],\n",
      "        [8]])\n",
      "Top-1 indices along dim=-1: tensor([[1],\n",
      "        [2],\n",
      "        [0]])\n"
     ]
    }
   ],
   "execution_count": 90
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
